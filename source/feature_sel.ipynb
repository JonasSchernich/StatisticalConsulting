{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Habe hier mal beispielhaft alle csv files geladen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import DataLoader\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "base_path = os.path.dirname(os.getcwd()) \n",
    "loader = DataLoader(base_path)\n",
    "loader.load_all_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expressions-Daten\n",
    "exprs_data = loader.exprs_data\n",
    "\n",
    "# Originale pData\n",
    "pdata_original = loader.pdata_original\n",
    "\n",
    "# Imputierte pData\n",
    "pdata_imputed = loader.pdata_imputed\n",
    "\n",
    "# All Genes Daten\n",
    "all_genes_data = loader.all_genes_data\n",
    "\n",
    "# Common Genes Daten\n",
    "common_genes_data = loader.common_genes_data\n",
    "\n",
    "# Intersection Daten\n",
    "intersection_data = loader.intersection_data\n",
    "\n",
    "# Merged originale pData\n",
    "merged_pdata_original = loader.merged_pdata_original\n",
    "\n",
    "# Merged imputierte pData\n",
    "merged_pdata_imputed = loader.merged_pdata_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENSG00000000003</th>\n",
       "      <th>ENSG00000000005</th>\n",
       "      <th>ENSG00000000419</th>\n",
       "      <th>ENSG00000000457</th>\n",
       "      <th>ENSG00000000460</th>\n",
       "      <th>ENSG00000000938</th>\n",
       "      <th>ENSG00000000971</th>\n",
       "      <th>ENSG00000001036</th>\n",
       "      <th>ENSG00000001084</th>\n",
       "      <th>ENSG00000001167</th>\n",
       "      <th>...</th>\n",
       "      <th>ENSG00000285230</th>\n",
       "      <th>ENSG00000285269</th>\n",
       "      <th>ENSG00000285294</th>\n",
       "      <th>ENSG00000286098</th>\n",
       "      <th>ENSG00000286112</th>\n",
       "      <th>ENSG00000288537</th>\n",
       "      <th>ENSG00000288547</th>\n",
       "      <th>ENSG00000288596</th>\n",
       "      <th>ENSG00000288611</th>\n",
       "      <th>ENSG00000288642</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>S1236-00001</th>\n",
       "      <td>0.991852</td>\n",
       "      <td>-0.614215</td>\n",
       "      <td>-0.613540</td>\n",
       "      <td>0.506800</td>\n",
       "      <td>-0.630192</td>\n",
       "      <td>-0.551416</td>\n",
       "      <td>-0.033500</td>\n",
       "      <td>0.697036</td>\n",
       "      <td>0.599751</td>\n",
       "      <td>0.737915</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999693</td>\n",
       "      <td>0.619542</td>\n",
       "      <td>1.020427</td>\n",
       "      <td>1.338751</td>\n",
       "      <td>-1.122334</td>\n",
       "      <td>-0.660175</td>\n",
       "      <td>-1.212611</td>\n",
       "      <td>1.400297</td>\n",
       "      <td>-1.444637</td>\n",
       "      <td>1.067246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S1236-00004</th>\n",
       "      <td>0.266909</td>\n",
       "      <td>-0.849028</td>\n",
       "      <td>0.087546</td>\n",
       "      <td>0.441891</td>\n",
       "      <td>0.029347</td>\n",
       "      <td>-1.757765</td>\n",
       "      <td>1.309211</td>\n",
       "      <td>0.361391</td>\n",
       "      <td>0.015141</td>\n",
       "      <td>0.569315</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.072023</td>\n",
       "      <td>0.011313</td>\n",
       "      <td>1.082162</td>\n",
       "      <td>-0.154773</td>\n",
       "      <td>-0.792405</td>\n",
       "      <td>-1.476893</td>\n",
       "      <td>-0.203524</td>\n",
       "      <td>-0.298842</td>\n",
       "      <td>-0.680895</td>\n",
       "      <td>-0.501956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S1236-00006</th>\n",
       "      <td>-0.141473</td>\n",
       "      <td>-0.783533</td>\n",
       "      <td>0.562788</td>\n",
       "      <td>-0.103213</td>\n",
       "      <td>0.018145</td>\n",
       "      <td>-0.540603</td>\n",
       "      <td>-0.330179</td>\n",
       "      <td>-0.380027</td>\n",
       "      <td>0.964349</td>\n",
       "      <td>-0.544147</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.423457</td>\n",
       "      <td>1.029345</td>\n",
       "      <td>-0.544842</td>\n",
       "      <td>0.426507</td>\n",
       "      <td>-0.415891</td>\n",
       "      <td>-0.654056</td>\n",
       "      <td>-0.194915</td>\n",
       "      <td>1.190055</td>\n",
       "      <td>-0.368631</td>\n",
       "      <td>0.606909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S1236-00007</th>\n",
       "      <td>0.006980</td>\n",
       "      <td>-0.538768</td>\n",
       "      <td>-0.442559</td>\n",
       "      <td>0.431006</td>\n",
       "      <td>-0.210979</td>\n",
       "      <td>0.554546</td>\n",
       "      <td>-0.471262</td>\n",
       "      <td>-1.983007</td>\n",
       "      <td>-1.919148</td>\n",
       "      <td>-1.997651</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.704783</td>\n",
       "      <td>1.624623</td>\n",
       "      <td>1.796287</td>\n",
       "      <td>-0.297364</td>\n",
       "      <td>-0.344096</td>\n",
       "      <td>1.069892</td>\n",
       "      <td>0.603428</td>\n",
       "      <td>-1.738376</td>\n",
       "      <td>0.747858</td>\n",
       "      <td>1.835464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S1236-00008</th>\n",
       "      <td>-0.105734</td>\n",
       "      <td>3.669036</td>\n",
       "      <td>0.529324</td>\n",
       "      <td>1.409513</td>\n",
       "      <td>0.039870</td>\n",
       "      <td>-0.037389</td>\n",
       "      <td>-0.178917</td>\n",
       "      <td>0.191722</td>\n",
       "      <td>-0.070154</td>\n",
       "      <td>0.168704</td>\n",
       "      <td>...</td>\n",
       "      <td>0.497249</td>\n",
       "      <td>0.003857</td>\n",
       "      <td>0.389307</td>\n",
       "      <td>-0.478628</td>\n",
       "      <td>0.020909</td>\n",
       "      <td>-0.413535</td>\n",
       "      <td>-0.999635</td>\n",
       "      <td>0.349303</td>\n",
       "      <td>-0.803112</td>\n",
       "      <td>0.428734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S1236-00500</th>\n",
       "      <td>-0.443601</td>\n",
       "      <td>-0.905781</td>\n",
       "      <td>1.566699</td>\n",
       "      <td>-0.049962</td>\n",
       "      <td>-0.540579</td>\n",
       "      <td>-0.834016</td>\n",
       "      <td>1.713012</td>\n",
       "      <td>-0.295689</td>\n",
       "      <td>0.260476</td>\n",
       "      <td>-1.534395</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.668967</td>\n",
       "      <td>-0.387030</td>\n",
       "      <td>-1.661804</td>\n",
       "      <td>-0.404159</td>\n",
       "      <td>-0.949696</td>\n",
       "      <td>-0.727479</td>\n",
       "      <td>-0.393544</td>\n",
       "      <td>0.718256</td>\n",
       "      <td>-0.099629</td>\n",
       "      <td>2.071877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S1236-00503</th>\n",
       "      <td>0.261961</td>\n",
       "      <td>-0.742918</td>\n",
       "      <td>-0.062466</td>\n",
       "      <td>0.076769</td>\n",
       "      <td>0.338240</td>\n",
       "      <td>1.081259</td>\n",
       "      <td>-0.046601</td>\n",
       "      <td>0.331313</td>\n",
       "      <td>0.196896</td>\n",
       "      <td>0.090985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.751865</td>\n",
       "      <td>0.018770</td>\n",
       "      <td>-0.996546</td>\n",
       "      <td>0.399427</td>\n",
       "      <td>1.368028</td>\n",
       "      <td>-0.000429</td>\n",
       "      <td>0.571226</td>\n",
       "      <td>-1.087196</td>\n",
       "      <td>-0.289985</td>\n",
       "      <td>0.895872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S1236-00505</th>\n",
       "      <td>0.749242</td>\n",
       "      <td>-0.188297</td>\n",
       "      <td>1.603825</td>\n",
       "      <td>0.608144</td>\n",
       "      <td>-0.883417</td>\n",
       "      <td>0.018892</td>\n",
       "      <td>1.344381</td>\n",
       "      <td>-0.106244</td>\n",
       "      <td>0.081580</td>\n",
       "      <td>1.530998</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.523104</td>\n",
       "      <td>1.193706</td>\n",
       "      <td>2.083624</td>\n",
       "      <td>-0.793513</td>\n",
       "      <td>-0.448226</td>\n",
       "      <td>-0.975384</td>\n",
       "      <td>-0.581971</td>\n",
       "      <td>-0.304262</td>\n",
       "      <td>1.030456</td>\n",
       "      <td>0.499336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S1236-00508</th>\n",
       "      <td>0.188972</td>\n",
       "      <td>0.054585</td>\n",
       "      <td>4.936192</td>\n",
       "      <td>1.079525</td>\n",
       "      <td>-1.224897</td>\n",
       "      <td>-0.755426</td>\n",
       "      <td>-1.172648</td>\n",
       "      <td>1.107656</td>\n",
       "      <td>1.475507</td>\n",
       "      <td>0.168267</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.489682</td>\n",
       "      <td>1.242213</td>\n",
       "      <td>0.565184</td>\n",
       "      <td>3.285181</td>\n",
       "      <td>-1.094931</td>\n",
       "      <td>0.107595</td>\n",
       "      <td>-1.256609</td>\n",
       "      <td>-0.088961</td>\n",
       "      <td>-0.585717</td>\n",
       "      <td>-0.278402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S1236-00510</th>\n",
       "      <td>-0.330337</td>\n",
       "      <td>0.659773</td>\n",
       "      <td>-1.601150</td>\n",
       "      <td>-0.423655</td>\n",
       "      <td>-0.505616</td>\n",
       "      <td>0.673154</td>\n",
       "      <td>-1.471040</td>\n",
       "      <td>-2.079740</td>\n",
       "      <td>-1.978643</td>\n",
       "      <td>-0.057899</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.475858</td>\n",
       "      <td>-0.750445</td>\n",
       "      <td>0.454904</td>\n",
       "      <td>0.389696</td>\n",
       "      <td>1.536829</td>\n",
       "      <td>-0.631903</td>\n",
       "      <td>0.634035</td>\n",
       "      <td>-1.499225</td>\n",
       "      <td>1.337158</td>\n",
       "      <td>-0.037415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>248 rows × 16810 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ENSG00000000003  ENSG00000000005  ENSG00000000419  \\\n",
       "S1236-00001         0.991852        -0.614215        -0.613540   \n",
       "S1236-00004         0.266909        -0.849028         0.087546   \n",
       "S1236-00006        -0.141473        -0.783533         0.562788   \n",
       "S1236-00007         0.006980        -0.538768        -0.442559   \n",
       "S1236-00008        -0.105734         3.669036         0.529324   \n",
       "...                      ...              ...              ...   \n",
       "S1236-00500        -0.443601        -0.905781         1.566699   \n",
       "S1236-00503         0.261961        -0.742918        -0.062466   \n",
       "S1236-00505         0.749242        -0.188297         1.603825   \n",
       "S1236-00508         0.188972         0.054585         4.936192   \n",
       "S1236-00510        -0.330337         0.659773        -1.601150   \n",
       "\n",
       "             ENSG00000000457  ENSG00000000460  ENSG00000000938  \\\n",
       "S1236-00001         0.506800        -0.630192        -0.551416   \n",
       "S1236-00004         0.441891         0.029347        -1.757765   \n",
       "S1236-00006        -0.103213         0.018145        -0.540603   \n",
       "S1236-00007         0.431006        -0.210979         0.554546   \n",
       "S1236-00008         1.409513         0.039870        -0.037389   \n",
       "...                      ...              ...              ...   \n",
       "S1236-00500        -0.049962        -0.540579        -0.834016   \n",
       "S1236-00503         0.076769         0.338240         1.081259   \n",
       "S1236-00505         0.608144        -0.883417         0.018892   \n",
       "S1236-00508         1.079525        -1.224897        -0.755426   \n",
       "S1236-00510        -0.423655        -0.505616         0.673154   \n",
       "\n",
       "             ENSG00000000971  ENSG00000001036  ENSG00000001084  \\\n",
       "S1236-00001        -0.033500         0.697036         0.599751   \n",
       "S1236-00004         1.309211         0.361391         0.015141   \n",
       "S1236-00006        -0.330179        -0.380027         0.964349   \n",
       "S1236-00007        -0.471262        -1.983007        -1.919148   \n",
       "S1236-00008        -0.178917         0.191722        -0.070154   \n",
       "...                      ...              ...              ...   \n",
       "S1236-00500         1.713012        -0.295689         0.260476   \n",
       "S1236-00503        -0.046601         0.331313         0.196896   \n",
       "S1236-00505         1.344381        -0.106244         0.081580   \n",
       "S1236-00508        -1.172648         1.107656         1.475507   \n",
       "S1236-00510        -1.471040        -2.079740        -1.978643   \n",
       "\n",
       "             ENSG00000001167  ...  ENSG00000285230  ENSG00000285269  \\\n",
       "S1236-00001         0.737915  ...        -0.999693         0.619542   \n",
       "S1236-00004         0.569315  ...        -1.072023         0.011313   \n",
       "S1236-00006        -0.544147  ...        -0.423457         1.029345   \n",
       "S1236-00007        -1.997651  ...        -1.704783         1.624623   \n",
       "S1236-00008         0.168704  ...         0.497249         0.003857   \n",
       "...                      ...  ...              ...              ...   \n",
       "S1236-00500        -1.534395  ...        -0.668967        -0.387030   \n",
       "S1236-00503         0.090985  ...         0.751865         0.018770   \n",
       "S1236-00505         1.530998  ...        -0.523104         1.193706   \n",
       "S1236-00508         0.168267  ...        -1.489682         1.242213   \n",
       "S1236-00510        -0.057899  ...        -0.475858        -0.750445   \n",
       "\n",
       "             ENSG00000285294  ENSG00000286098  ENSG00000286112  \\\n",
       "S1236-00001         1.020427         1.338751        -1.122334   \n",
       "S1236-00004         1.082162        -0.154773        -0.792405   \n",
       "S1236-00006        -0.544842         0.426507        -0.415891   \n",
       "S1236-00007         1.796287        -0.297364        -0.344096   \n",
       "S1236-00008         0.389307        -0.478628         0.020909   \n",
       "...                      ...              ...              ...   \n",
       "S1236-00500        -1.661804        -0.404159        -0.949696   \n",
       "S1236-00503        -0.996546         0.399427         1.368028   \n",
       "S1236-00505         2.083624        -0.793513        -0.448226   \n",
       "S1236-00508         0.565184         3.285181        -1.094931   \n",
       "S1236-00510         0.454904         0.389696         1.536829   \n",
       "\n",
       "             ENSG00000288537  ENSG00000288547  ENSG00000288596  \\\n",
       "S1236-00001        -0.660175        -1.212611         1.400297   \n",
       "S1236-00004        -1.476893        -0.203524        -0.298842   \n",
       "S1236-00006        -0.654056        -0.194915         1.190055   \n",
       "S1236-00007         1.069892         0.603428        -1.738376   \n",
       "S1236-00008        -0.413535        -0.999635         0.349303   \n",
       "...                      ...              ...              ...   \n",
       "S1236-00500        -0.727479        -0.393544         0.718256   \n",
       "S1236-00503        -0.000429         0.571226        -1.087196   \n",
       "S1236-00505        -0.975384        -0.581971        -0.304262   \n",
       "S1236-00508         0.107595        -1.256609        -0.088961   \n",
       "S1236-00510        -0.631903         0.634035        -1.499225   \n",
       "\n",
       "             ENSG00000288611  ENSG00000288642  \n",
       "S1236-00001        -1.444637         1.067246  \n",
       "S1236-00004        -0.680895        -0.501956  \n",
       "S1236-00006        -0.368631         0.606909  \n",
       "S1236-00007         0.747858         1.835464  \n",
       "S1236-00008        -0.803112         0.428734  \n",
       "...                      ...              ...  \n",
       "S1236-00500        -0.099629         2.071877  \n",
       "S1236-00503        -0.289985         0.895872  \n",
       "S1236-00505         1.030456         0.499336  \n",
       "S1236-00508        -0.585717        -0.278402  \n",
       "S1236-00510         1.337158        -0.037415  \n",
       "\n",
       "[248 rows x 16810 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exprs_data['Belfast_2018_Jain.csv']\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data = exprs_data['Belfast_2018_Jain.csv']\n",
    "# Schritt 2: Führe die Standardisierung durch\n",
    "test = scaler.fit_transform(data)\n",
    "test\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sehr krasse Korrelation zwischen viele genen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korrelation_groesser_als_07 = intersection_data['exprs_intersect.csv'].corr() > 0.9\n",
    "korrelation_groesser_als_07.values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import DataLoader\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sksurv.linear_model import CoxnetSurvivalAnalysis\n",
    "from sksurv.util import Surv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sksurv.metrics import concordance_index_censored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cox proportinal hazard für feature selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jl/__x0ybfj19qbrwgn16qjksvh0000gn/T/ipykernel_31136/408709216.py:19: ConvergenceWarning: Optimization terminated early, you might want to increase the number of iterations (max_iter=1000).\n",
      "  cox_model.fit(X_train, y_train)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CoxnetSurvivalAnalysis(l1_ratio=1.0, max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;CoxnetSurvivalAnalysis<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>CoxnetSurvivalAnalysis(l1_ratio=1.0, max_iter=1000)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "CoxnetSurvivalAnalysis(l1_ratio=1.0, max_iter=1000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Cox Proportional Hazards Regression mit Lasso-Regularisierung\n",
    "\n",
    "# Laden der gemergten und imputierten Daten\n",
    "pdata = loader.merged_pdata_imputed['merged_imputed_pData.csv']\n",
    "exprs = loader.common_genes_data['common_genes_knn_imputed.csv']\n",
    "\n",
    "# Entfernen von Zeilen mit MONTH_TO_BCR = 0\n",
    "valid_indices = pdata[pdata['MONTH_TO_BCR'] > 0].index\n",
    "pdata = pdata.loc[valid_indices]\n",
    "exprs = exprs.loc[valid_indices]\n",
    "\n",
    "y = Surv.from_arrays(event=pdata['BCR_STATUS'], time=pdata['MONTH_TO_BCR'])\n",
    "\n",
    "# Teilen der Daten in Trainings- und Testsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(exprs, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Erstellen und Trainieren des Modells\n",
    "cox_model = CoxnetSurvivalAnalysis(l1_ratio=1.0, max_iter=1000)\n",
    "cox_model.fit(X_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl ausgewählter Features: 1338\n",
      "\n",
      "Cox Regression C-index auf Testdaten: 0.574\n"
     ]
    }
   ],
   "source": [
    "# Feature-Auswahl basierend auf Nicht-Null-Koeffizienten\n",
    "def select_features_by_non_zero_coef(model, X, threshold=1e-5):\n",
    "    coef = model.coef_\n",
    "    \n",
    "    \n",
    "    # Wenn coef mehrdimensional ist, nehmen wir den Mittelwert über alle Dimensionen\n",
    "    if coef.ndim > 1:\n",
    "        coef = np.mean(np.abs(coef), axis=1)\n",
    "    else:\n",
    "        coef = np.abs(coef)\n",
    "    \n",
    "    selected_features = X.columns[coef > threshold]\n",
    "    return selected_features\n",
    "\n",
    "selected_features_cox = select_features_by_non_zero_coef(cox_model, X_train)\n",
    "print(f\"Anzahl ausgewählter Features: {len(selected_features_cox)}\\n\")\n",
    "\n",
    "# Berechnen des C-index auf den Testdaten\n",
    "cox_c_index = cox_model.score(X_test, y_test)\n",
    "print(f\"Cox Regression C-index auf Testdaten: {cox_c_index:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest (keine survival Zeit) für feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl ausgewählter Features: 1097\n",
      "AUC-ROC auf Testdaten: 0.644\n",
      "\n",
      "Anzahl ausgewählter Features: 347\n",
      "AUC-ROC auf Testdaten: 0.212\n",
      "\n",
      "Anzahl ausgewählter Features: 307\n",
      "AUC-ROC auf Testdaten: 0.846\n",
      "\n",
      "Anzahl ausgewählter Features: 602\n",
      "AUC-ROC auf Testdaten: 0.891\n",
      "\n",
      "Anzahl ausgewählter Features: 546\n",
      "AUC-ROC auf Testdaten: 0.725\n",
      "\n",
      "Anzahl ausgewählter Features: 519\n",
      "AUC-ROC auf Testdaten: 0.833\n",
      "\n",
      "Anzahl ausgewählter Features: 391\n",
      "AUC-ROC auf Testdaten: 0.717\n",
      "\n",
      "Anzahl ausgewählter Features: 471\n",
      "AUC-ROC auf Testdaten: 0.615\n",
      "\n",
      "Anzahl ausgewählter Features: 539\n",
      "AUC-ROC auf Testdaten: 0.723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. Random Forest mit Feature Importance\n",
    "\n",
    "def random_forest_for_cohort(pdata, exprs):\n",
    "    # Entfernen von Zeilen mit MONTH_TO_BCR = 0\n",
    "    valid_indices = pdata[pdata['MONTH_TO_BCR'] > 0].index\n",
    "    pdata = pdata.loc[valid_indices]\n",
    "    exprs = exprs.loc[valid_indices]\n",
    "    \n",
    "    y = pdata['BCR_STATUS']\n",
    "    \n",
    "    # Teilen der Daten in Trainings- und Testsets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(exprs, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Erstellen und Trainieren des Modells\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Berechnen der Feature Importance\n",
    "    feature_importance = pd.Series(model.feature_importances_, index=exprs.columns).sort_values(ascending=False)\n",
    "    \n",
    "    # Feature-Auswahl basierend auf kumulativer Wichtigkeit\n",
    "    cumulative_importance = np.cumsum(feature_importance) / np.sum(feature_importance)\n",
    "    selected_features = feature_importance[cumulative_importance <= 0.95].index\n",
    "    \n",
    "    # Berechnen des AUC-ROC auf den Testdaten\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    return model, feature_importance, selected_features, auc_roc\n",
    "\n",
    "# Durchführen der Random Forest Analyse für jede Kohorte\n",
    "rf_results = {}\n",
    "for cohort, pdata in loader.pdata_original.items():\n",
    "    exprs = loader.exprs_data[cohort]\n",
    "    model, feature_importance, selected_features, auc_roc = random_forest_for_cohort(pdata, exprs)\n",
    "    \n",
    "    rf_results[cohort] = {\n",
    "        'model': model,\n",
    "        'feature_importance': feature_importance,\n",
    "        'selected_features': selected_features,\n",
    "        'auc_roc': auc_roc\n",
    "    }\n",
    "    \n",
    "    print(f\"Anzahl ausgewählter Features: {len(selected_features)}\")\n",
    "    print(f\"AUC-ROC auf Testdaten: {auc_roc:.3f}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of selected features: 5\n",
      "Shape of expression data: (1091, 5)\n",
      "Shape of survival data: (1091, 12)\n",
      "Number of common indices: 1091\n",
      "Random Survival Forest C-index (with selected features): 0.6565\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def select_features_with_rf(pdata_dict, exprs_dict, common_genes, importance_threshold=0.95):\n",
    "    \"\"\"\n",
    "    Perform feature selection using Random Forest for each cohort\n",
    "    and combine the results, ensuring selected features exist in common genes.\n",
    "    \"\"\"\n",
    "    all_selected_features = []\n",
    "    \n",
    "    for cohort, pdata in pdata_dict.items():\n",
    "        # Skip if no expression data available\n",
    "        if cohort not in exprs_dict:\n",
    "            continue\n",
    "            \n",
    "        exprs = exprs_dict[cohort]\n",
    "        \n",
    "        # Only consider genes that are in common_genes\n",
    "        available_genes = list(set(exprs.columns) & set(common_genes.columns))\n",
    "        exprs = exprs[available_genes]\n",
    "        \n",
    "        # Remove samples with MONTH_TO_BCR = 0\n",
    "        valid_indices = pdata[pdata['MONTH_TO_BCR'] > 0].index\n",
    "        pdata_filtered = pdata.loc[valid_indices]\n",
    "        exprs_filtered = exprs.loc[valid_indices]\n",
    "        \n",
    "        y = pdata_filtered['BCR_STATUS']\n",
    "        \n",
    "        # Train Random Forest\n",
    "        rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        rf_model.fit(exprs_filtered, y)\n",
    "        \n",
    "        # Calculate feature importance\n",
    "        feature_importance = pd.Series(\n",
    "            rf_model.feature_importances_, \n",
    "            index=exprs_filtered.columns\n",
    "        ).sort_values(ascending=False)\n",
    "        \n",
    "        # Select features based on cumulative importance\n",
    "        cumulative_importance = np.cumsum(feature_importance) / np.sum(feature_importance)\n",
    "        selected_features = feature_importance[cumulative_importance <= importance_threshold].index.tolist()\n",
    "        \n",
    "        all_selected_features.extend(selected_features)\n",
    "    \n",
    "    # Count feature occurrences across cohorts\n",
    "    feature_counts = Counter(all_selected_features)\n",
    "    \n",
    "    # Select features that appear in at least half of the cohorts\n",
    "    min_cohorts = max(1, len(pdata_dict) // 2)\n",
    "    final_features = [feature for feature, count in feature_counts.items() \n",
    "                     if count >= min_cohorts]\n",
    "    \n",
    "    # Ensure all selected features are in common genes\n",
    "    final_features = list(set(final_features) & set(common_genes.columns))\n",
    "    \n",
    "    return final_features\n",
    "\n",
    "def prepare_survival_data(X, survival_data):\n",
    "    \"\"\"\n",
    "    Prepare data for survival analysis.\n",
    "    \"\"\"\n",
    "    # Ensure index alignment\n",
    "    common_index = X.index.intersection(survival_data.index)\n",
    "    X = X.loc[common_index]\n",
    "    survival_data = survival_data.loc[common_index]\n",
    "    \n",
    "    merged_data = pd.concat([X, survival_data[['MONTH_TO_BCR', 'BCR_STATUS']]], axis=1)\n",
    "    X = merged_data.drop(['MONTH_TO_BCR', 'BCR_STATUS'], axis=1)\n",
    "    \n",
    "    # Create structured array for survival data\n",
    "    y = np.array([(bool(status), time) for status, time in \n",
    "                  zip(merged_data['BCR_STATUS'], merged_data['MONTH_TO_BCR'])],\n",
    "                 dtype=[('status', bool), ('time', float)])\n",
    "    \n",
    "    return train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def fit_random_survival_forest(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Fit Random Survival Forest and evaluate performance.\n",
    "    \"\"\"\n",
    "    rsf = RandomSurvivalForest(n_estimators=100, random_state=42)\n",
    "    rsf.fit(X_train, y_train)\n",
    "    \n",
    "    predictions = rsf.predict(X_test)\n",
    "    c_index = concordance_index_censored(\n",
    "        y_test['status'], \n",
    "        y_test['time'], \n",
    "        predictions\n",
    "    )[0]\n",
    "    \n",
    "    return rsf, c_index\n",
    "\n",
    "def run_complete_analysis(loader, merged_pdata_imputed):\n",
    "    \"\"\"\n",
    "    Run the complete analysis pipeline.\n",
    "    \"\"\"\n",
    "    # Get common genes data first\n",
    "    common_genes = loader.common_genes_data['common_genes_knn_imputed.csv']\n",
    "    \n",
    "    # 1. Feature Selection using Random Forest\n",
    "    selected_features = select_features_with_rf(\n",
    "        loader.pdata_original,\n",
    "        loader.exprs_data,\n",
    "        common_genes\n",
    "    )\n",
    "    print(f\"Number of selected features: {len(selected_features)}\")\n",
    "    \n",
    "    # 2. Prepare data for survival analysis\n",
    "    exprs = common_genes\n",
    "    survival_data = merged_pdata_imputed['merged_imputed_pData.csv']\n",
    "    \n",
    "    # Filter expression data to only include selected features\n",
    "    exprs_selected = exprs[selected_features]\n",
    "    \n",
    "    # Print some diagnostics\n",
    "    print(f\"Shape of expression data: {exprs_selected.shape}\")\n",
    "    print(f\"Shape of survival data: {survival_data.shape}\")\n",
    "    print(f\"Number of common indices: {len(exprs_selected.index.intersection(survival_data.index))}\")\n",
    "    \n",
    "    # 3. Prepare and split data\n",
    "    X_train, X_test, y_train, y_test = prepare_survival_data(\n",
    "        exprs_selected, \n",
    "        survival_data\n",
    "    )\n",
    "    \n",
    "    # 4. Fit Random Survival Forest and evaluate\n",
    "    rsf_model, c_index = fit_random_survival_forest(\n",
    "        X_train, y_train, X_test, y_test\n",
    "    )\n",
    "    \n",
    "    print(f\"Random Survival Forest C-index (with selected features): {c_index:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'selected_features': selected_features,\n",
    "        'rsf_model': rsf_model,\n",
    "        'c_index': c_index,\n",
    "        'training_data': (X_train, y_train),\n",
    "        'test_data': (X_test, y_test)\n",
    "    }\n",
    "\n",
    "results = run_complete_analysis(loader, merged_pdata_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random surivival forest mit feature importance(dauert fast eine Stunde)\n",
    "C-index auf Testdaten: 0.670\n",
    "Anzahl der ausgewählten Features: 14691\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 72\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m final_model, c_index, selected_features\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Durchführen der Random Survival Forest Analyse mit Feature-Selektion\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m model, c_index, selected_features \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_survival_forest_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimportance_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC-index auf Testdaten: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc_index\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnzahl der ausgewählten Features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(selected_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 48\u001b[0m, in \u001b[0;36mrandom_survival_forest_analysis\u001b[0;34m(pdata, exprs, importance_threshold)\u001b[0m\n\u001b[1;32m     45\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(exprs, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Feature-Selektion\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m selected_features \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimportance_threshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Filtern der Daten auf die ausgewählten Features\u001b[39;00m\n\u001b[1;32m     51\u001b[0m X_train_selected \u001b[38;5;241m=\u001b[39m X_train[selected_features]\n",
      "Cell \u001b[0;32mIn[19], line 13\u001b[0m, in \u001b[0;36mfeature_selection\u001b[0;34m(X, y, importance_threshold)\u001b[0m\n\u001b[1;32m     11\u001b[0m rsf \u001b[38;5;241m=\u001b[39m RandomSurvivalForest(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, min_samples_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, min_samples_leaf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     12\u001b[0m rsf\u001b[38;5;241m.\u001b[39mfit(X[[feature]], y)\n\u001b[0;32m---> 13\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m rsf\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     14\u001b[0m c_index \u001b[38;5;241m=\u001b[39m concordance_index_censored(y[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStatus\u001b[39m\u001b[38;5;124m'\u001b[39m], y[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m'\u001b[39m], y_pred)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     15\u001b[0m c_indices\u001b[38;5;241m.\u001b[39mappend((feature, c_index))\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Finanzen/Lev Strategy/StatisticalConsulting/.venv/lib/python3.11/site-packages/pandas/core/frame.py:4117\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(indexer, \u001b[38;5;28mslice\u001b[39m):\n\u001b[1;32m   4115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice(indexer, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 4117\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_with_is_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_single_key:\n\u001b[1;32m   4120\u001b[0m     \u001b[38;5;66;03m# What does looking for a single key in a non-unique index return?\u001b[39;00m\n\u001b[1;32m   4121\u001b[0m     \u001b[38;5;66;03m# The behavior is inconsistent. It returns a Series, except when\u001b[39;00m\n\u001b[1;32m   4122\u001b[0m     \u001b[38;5;66;03m# - the key itself is repeated (test on data.shape, #9519), or\u001b[39;00m\n\u001b[1;32m   4123\u001b[0m     \u001b[38;5;66;03m# - we have a MultiIndex on columns (test on self.columns, #21309)\u001b[39;00m\n\u001b[1;32m   4124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n\u001b[1;32m   4125\u001b[0m         \u001b[38;5;66;03m# GH#26490 using data[key] can cause RecursionError\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Finanzen/Lev Strategy/StatisticalConsulting/.venv/lib/python3.11/site-packages/pandas/core/generic.py:4153\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m   4142\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   4143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_with_is_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices, axis: Axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   4144\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4145\u001b[0m \u001b[38;5;124;03m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[1;32m   4146\u001b[0m \u001b[38;5;124;03m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4151\u001b[0m \u001b[38;5;124;03m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[1;32m   4152\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4153\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4154\u001b[0m     \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[1;32m   4155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39m_get_axis(axis)\u001b[38;5;241m.\u001b[39mequals(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(axis)):\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Finanzen/Lev Strategy/StatisticalConsulting/.venv/lib/python3.11/site-packages/pandas/core/generic.py:4133\u001b[0m, in \u001b[0;36mNDFrame.take\u001b[0;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[1;32m   4128\u001b[0m     \u001b[38;5;66;03m# We can get here with a slice via DataFrame.__getitem__\u001b[39;00m\n\u001b[1;32m   4129\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\n\u001b[1;32m   4130\u001b[0m         indices\u001b[38;5;241m.\u001b[39mstart, indices\u001b[38;5;241m.\u001b[39mstop, indices\u001b[38;5;241m.\u001b[39mstep, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp\n\u001b[1;32m   4131\u001b[0m     )\n\u001b[0;32m-> 4133\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4135\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4137\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[1;32m   4139\u001b[0m     \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4140\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Finanzen/Lev Strategy/StatisticalConsulting/.venv/lib/python3.11/site-packages/pandas/core/internals/managers.py:894\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[0;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[1;32m    891\u001b[0m indexer \u001b[38;5;241m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[38;5;241m=\u001b[39mverify)\n\u001b[1;32m    893\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Finanzen/Lev Strategy/StatisticalConsulting/.venv/lib/python3.11/site-packages/pandas/core/internals/managers.py:680\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested axis not found in manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 680\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_slice_take_blocks_ax0\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43monly_slice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monly_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_na_proxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_na_proxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    687\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    688\u001b[0m         blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[1;32m    689\u001b[0m             indexer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[1;32m    696\u001b[0m     ]\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Finanzen/Lev Strategy/StatisticalConsulting/.venv/lib/python3.11/site-packages/pandas/core/internals/managers.py:843\u001b[0m, in \u001b[0;36mBaseBlockManager._slice_take_blocks_ax0\u001b[0;34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy, ref_inplace_op)\u001b[0m\n\u001b[1;32m    841\u001b[0m                     blocks\u001b[38;5;241m.\u001b[39mappend(nb)\n\u001b[1;32m    842\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 843\u001b[0m                 nb \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_mgr_locs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmgr_locs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m                 blocks\u001b[38;5;241m.\u001b[39mappend(nb)\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m blocks\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Finanzen/Lev Strategy/StatisticalConsulting/.venv/lib/python3.11/site-packages/pandas/core/internals/blocks.py:1307\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[1;32m   1304\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[0;32m-> 1307\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43malgos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[1;32m   1314\u001b[0m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Finanzen/Lev Strategy/StatisticalConsulting/.venv/lib/python3.11/site-packages/pandas/core/array_algos/take.py:97\u001b[0m, in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m     96\u001b[0m     fill_value \u001b[38;5;241m=\u001b[39m na_value_for_dtype(arr\u001b[38;5;241m.\u001b[39mdtype, compat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_np_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     98\u001b[0m     dtype, fill_value \u001b[38;5;241m=\u001b[39m maybe_promote(arr\u001b[38;5;241m.\u001b[39mdtype, fill_value)\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m dtype:\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;66;03m# EA.take is strict about returning a new object of the same type\u001b[39;00m\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;66;03m# so for that case cast upfront\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def feature_selection(X, y, importance_threshold=0.95):\n",
    "    c_indices = []\n",
    "    for feature in X.columns:\n",
    "        rsf = RandomSurvivalForest(n_estimators=10, min_samples_split=10, min_samples_leaf=5, random_state=42)\n",
    "        rsf.fit(X[[feature]], y)\n",
    "        y_pred = rsf.predict(X[[feature]])\n",
    "        c_index = concordance_index_censored(y['Status'], y['Time'], y_pred)[0]\n",
    "        c_indices.append((feature, c_index))\n",
    "    \n",
    "    c_indices.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Normalisieren der C-Indizes\n",
    "    total_c_index = sum(c_index for _, c_index in c_indices)\n",
    "    normalized_c_indices = [(feature, c_index / total_c_index) for feature, c_index in c_indices]\n",
    "    \n",
    "    # Berechnen der kumulativen Summe der normalisierten C-Indizes\n",
    "    cumulative_importance = np.cumsum([c_index for _, c_index in normalized_c_indices])\n",
    "    \n",
    "    # Finden des Index, an dem die kumulative Wichtigkeit den Schwellenwert überschreitet\n",
    "    importance_threshold_index = np.searchsorted(cumulative_importance, importance_threshold)\n",
    "    \n",
    "    # Auswählen der Features\n",
    "    selected_features = [feature for feature, _ in normalized_c_indices[:importance_threshold_index + 1]]\n",
    "    \n",
    "    return selected_features\n",
    "\n",
    "def random_survival_forest_analysis(pdata, exprs, importance_threshold=0.95):\n",
    "    # Entfernen von Zeilen mit MONTH_TO_BCR = 0\n",
    "    valid_indices = pdata[pdata['MONTH_TO_BCR'] > 0].index\n",
    "    pdata = pdata.loc[valid_indices]\n",
    "    exprs = exprs.loc[valid_indices]\n",
    "    \n",
    "    # Erstellen des strukturierten Arrays für die Überlebensdaten\n",
    "    y = np.array([(event, time) for event, time in zip(pdata['BCR_STATUS'], pdata['MONTH_TO_BCR'])],\n",
    "                 dtype=[('Status', bool), ('Time', float)])\n",
    "    \n",
    "    # Teilen der Daten in Trainings- und Testsets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(exprs, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Feature-Selektion\n",
    "    selected_features = feature_selection(X_train, y_train, importance_threshold)\n",
    "    \n",
    "    # Filtern der Daten auf die ausgewählten Features\n",
    "    X_train_selected = X_train[selected_features]\n",
    "    X_test_selected = X_test[selected_features]\n",
    "    \n",
    "    # Skalieren der Daten\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "    X_test_scaled = scaler.transform(X_test_selected)\n",
    "    \n",
    "    # Erstellen und Trainieren des finalen Modells mit den ausgewählten Features\n",
    "    final_model = RandomSurvivalForest(n_estimators=100, min_samples_split=10, min_samples_leaf=5, random_state=42)\n",
    "    final_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Berechnen des C-index auf den Testdaten\n",
    "    y_pred = final_model.predict(X_test_scaled)\n",
    "    print(\"Range of predictions:\", y_pred.min(), \"-\", y_pred.max())\n",
    "    \n",
    "    c_index = concordance_index_censored(y_test['Status'], y_test['Time'], y_pred)[0]\n",
    "    \n",
    "    return final_model, c_index, selected_features\n",
    "\n",
    "# Durchführen der Random Survival Forest Analyse mit Feature-Selektion\n",
    "model, c_index, selected_features = random_survival_forest_analysis(pdata, exprs, importance_threshold=0.95)\n",
    "\n",
    "print(f\"C-index auf Testdaten: {c_index:.3f}\")\n",
    "print(f\"Anzahl der ausgewählten Features: {len(selected_features)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA für feature selection. Noch nicht sicher ob das so richtig gut ist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA - Anzahl der Hauptkomponenten, die 95% der Varianz erklären: 751\n",
      "PCA - Erste 5 Zeilen der reduzierten Daten:\n",
      "                               PC1        PC2        PC3        PC4  \\\n",
      "Atlanta_2014_Long.PT081   8.661650 -54.719946  -4.092102  16.679300   \n",
      "Atlanta_2014_Long.PT127  12.859319 -15.372824  22.981957 -10.651197   \n",
      "Atlanta_2014_Long.PT168  12.863487  -2.365458  18.698305  -1.895566   \n",
      "Atlanta_2014_Long.PT184  -6.229881 -28.850186  -7.283315 -21.765291   \n",
      "Atlanta_2014_Long.PT199  23.796958  -3.912335   9.055037 -16.155162   \n",
      "\n",
      "                               PC5        PC6        PC7        PC8  \\\n",
      "Atlanta_2014_Long.PT081 -81.804150 -64.164344 -37.145831 -23.252703   \n",
      "Atlanta_2014_Long.PT127  -0.881686  12.672881   3.627623   7.325505   \n",
      "Atlanta_2014_Long.PT168  -2.159397   4.539271   0.501976  14.035236   \n",
      "Atlanta_2014_Long.PT184  25.322421  14.117116   1.168117   7.638734   \n",
      "Atlanta_2014_Long.PT199  10.472695   1.362202  -1.278242   8.137241   \n",
      "\n",
      "                               PC9       PC10  ...     PC742     PC743  \\\n",
      "Atlanta_2014_Long.PT081 -28.000637  15.380086  ...  0.861004 -0.303478   \n",
      "Atlanta_2014_Long.PT127  -4.115635 -12.206580  ...  1.089648  0.003996   \n",
      "Atlanta_2014_Long.PT168  -2.054353  -7.063735  ... -0.565564 -0.797927   \n",
      "Atlanta_2014_Long.PT184  -2.982973   3.299297  ... -0.378526 -0.983678   \n",
      "Atlanta_2014_Long.PT199  -5.734324  -0.114022  ...  3.928149 -4.524384   \n",
      "\n",
      "                            PC744     PC745     PC746     PC747     PC748  \\\n",
      "Atlanta_2014_Long.PT081  0.532046  0.078278  0.130424  0.540414  0.213930   \n",
      "Atlanta_2014_Long.PT127 -0.366995 -2.197884  0.935422 -0.271411  0.789620   \n",
      "Atlanta_2014_Long.PT168  2.088677  1.752322  0.162703 -0.462825 -2.382273   \n",
      "Atlanta_2014_Long.PT184 -0.714063  1.947583  3.121380 -0.775789 -0.995760   \n",
      "Atlanta_2014_Long.PT199  4.642553 -0.309169 -2.840324  1.144181  0.777551   \n",
      "\n",
      "                            PC749     PC750     PC751  \n",
      "Atlanta_2014_Long.PT081 -0.734270  0.627856  0.274445  \n",
      "Atlanta_2014_Long.PT127 -1.265398  1.125642  0.171208  \n",
      "Atlanta_2014_Long.PT168  0.644229 -0.942000  0.469106  \n",
      "Atlanta_2014_Long.PT184  0.314737  1.876893 -0.041281  \n",
      "Atlanta_2014_Long.PT199 -2.489864  0.172245  2.308303  \n",
      "\n",
      "[5 rows x 751 columns]\n"
     ]
    }
   ],
   "source": [
    "# 3. Principal Component Analysis (PCA)\n",
    "\n",
    "# Laden der gemergten Daten mit gemeinsamen Genen\n",
    "exprs = loader.intersection_data['exprs_intersect.csv']\n",
    "pdata = loader.merged_pdata_original['merged_original_pData.csv']\n",
    "\n",
    "# Entfernen von Zeilen mit MONTH_TO_BCR = 0\n",
    "valid_indices = pdata[pdata['MONTH_TO_BCR'] > 0].index\n",
    "pdata = pdata.loc[valid_indices]\n",
    "exprs = exprs.loc[valid_indices]\n",
    "\n",
    "# Durchführen der PCA direkt auf den nicht standardisierten Daten\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(exprs)\n",
    "\n",
    "# Berechnen des erklärten Varianzanteils\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Plotten des erklärten Varianzanteils\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, 'bo-')\n",
    "plt.xlabel('Anzahl der Hauptkomponenten')\n",
    "plt.ylabel('Kumulativer erklärter Varianzanteil')\n",
    "plt.title('Erklärter Varianzanteil der Hauptkomponenten')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Wählen der Anzahl der Komponenten, die 95% der Varianz erklären\n",
    "n_components = np.argmax(cumulative_variance_ratio >= 0.95) + 1\n",
    "print(f\"PCA - Anzahl der Hauptkomponenten, die 95% der Varianz erklären: {n_components}\")\n",
    "\n",
    "# Reduzieren der Dimensionalität auf die ausgewählte Anzahl von Komponenten\n",
    "pca_reduced = PCA(n_components=n_components)\n",
    "X_pca_reduced = pca_reduced.fit_transform(exprs)\n",
    "\n",
    "# Erstellen eines DataFrames mit den reduzierten Daten\n",
    "pca_df = pd.DataFrame(X_pca_reduced, columns=[f'PC{i+1}' for i in range(n_components)], index=exprs.index)\n",
    "\n",
    "print(\"PCA - Erste 5 Zeilen der reduzierten Daten:\")\n",
    "print(pca_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unsupervised Variational Auto Encoder zur Variablenselection\n",
    "Daten musste ich nochmal normalisiern-> ob wir sie wirklich normalisiert bekommen haben und was schief läuft\n",
    "ggf kann man noch ein bisschen tunen und performance damit verbessern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 9163.6084\n",
      "Epoch 2, Loss: 9162.1290\n",
      "Epoch 3, Loss: 9160.8234\n",
      "Epoch 4, Loss: 9159.5588\n",
      "Epoch 5, Loss: 9158.3064\n",
      "Epoch 6, Loss: 9157.0218\n",
      "Epoch 7, Loss: 9155.6649\n",
      "Epoch 8, Loss: 9154.2358\n",
      "Epoch 9, Loss: 9152.6692\n",
      "Epoch 10, Loss: 9150.9396\n",
      "Epoch 11, Loss: 9149.0451\n",
      "Epoch 12, Loss: 9146.9924\n",
      "Epoch 13, Loss: 9144.7514\n",
      "Epoch 14, Loss: 9142.1940\n",
      "Epoch 15, Loss: 9139.6526\n",
      "Epoch 16, Loss: 9136.7074\n",
      "Epoch 17, Loss: 9133.6917\n",
      "Epoch 18, Loss: 9130.5985\n",
      "Epoch 19, Loss: 9127.1582\n",
      "Epoch 20, Loss: 9123.8948\n",
      "Epoch 21, Loss: 9120.4556\n",
      "Epoch 22, Loss: 9117.0837\n",
      "Epoch 23, Loss: 9113.9013\n",
      "Epoch 24, Loss: 9110.8768\n",
      "Epoch 25, Loss: 9108.1355\n",
      "Epoch 26, Loss: 9105.6413\n",
      "Epoch 27, Loss: 9103.4837\n",
      "Epoch 28, Loss: 9101.6053\n",
      "Epoch 29, Loss: 9099.9882\n",
      "Epoch 30, Loss: 9098.7832\n",
      "Epoch 31, Loss: 9097.6028\n",
      "Epoch 32, Loss: 9096.6448\n",
      "Epoch 33, Loss: 9095.7278\n",
      "Epoch 34, Loss: 9095.1394\n",
      "Epoch 35, Loss: 9094.6211\n",
      "Epoch 36, Loss: 9094.0339\n",
      "Epoch 37, Loss: 9093.7266\n",
      "Epoch 38, Loss: 9093.4080\n",
      "Epoch 39, Loss: 9092.9986\n",
      "Epoch 40, Loss: 9092.8999\n",
      "Epoch 41, Loss: 9092.5753\n",
      "Epoch 42, Loss: 9092.4811\n",
      "Epoch 43, Loss: 9092.2788\n",
      "Epoch 44, Loss: 9092.1598\n",
      "Epoch 45, Loss: 9092.0186\n",
      "Epoch 46, Loss: 9091.9001\n",
      "Epoch 47, Loss: 9091.8729\n",
      "Epoch 48, Loss: 9091.8289\n",
      "Epoch 49, Loss: 9091.5199\n",
      "Epoch 50, Loss: 9091.5953\n",
      "Epoch 51, Loss: 9091.5046\n",
      "Epoch 52, Loss: 9091.4635\n",
      "Epoch 53, Loss: 9091.3728\n",
      "Epoch 54, Loss: 9091.3731\n",
      "Epoch 55, Loss: 9091.2713\n",
      "Epoch 56, Loss: 9091.2106\n",
      "Epoch 57, Loss: 9091.2926\n",
      "Epoch 58, Loss: 9091.0637\n",
      "Epoch 59, Loss: 9091.2616\n",
      "Epoch 60, Loss: 9091.0277\n",
      "Epoch 61, Loss: 9091.0250\n",
      "Epoch 62, Loss: 9091.0372\n",
      "Epoch 63, Loss: 9091.0032\n",
      "Epoch 64, Loss: 9090.8935\n",
      "Epoch 65, Loss: 9090.8747\n",
      "Epoch 66, Loss: 9090.8349\n",
      "Epoch 67, Loss: 9090.7105\n",
      "Epoch 68, Loss: 9090.6742\n",
      "Epoch 69, Loss: 9090.6256\n",
      "Epoch 70, Loss: 9090.7368\n",
      "Epoch 71, Loss: 9090.6613\n",
      "Epoch 72, Loss: 9090.6331\n",
      "Epoch 73, Loss: 9090.7140\n",
      "Epoch 74, Loss: 9090.5489\n",
      "Epoch 75, Loss: 9090.4987\n",
      "Epoch 76, Loss: 9090.5061\n",
      "Epoch 77, Loss: 9090.4655\n",
      "Epoch 78, Loss: 9090.5740\n",
      "Epoch 79, Loss: 9090.3590\n",
      "Epoch 80, Loss: 9090.3884\n",
      "Epoch 81, Loss: 9090.5050\n",
      "Epoch 82, Loss: 9090.4011\n",
      "Epoch 83, Loss: 9090.3836\n",
      "Epoch 84, Loss: 9090.3991\n",
      "Epoch 85, Loss: 9090.3079\n",
      "Epoch 86, Loss: 9090.3347\n",
      "Epoch 87, Loss: 9090.2592\n",
      "Epoch 88, Loss: 9090.3277\n",
      "Epoch 89, Loss: 9090.2010\n",
      "Epoch 90, Loss: 9090.1144\n",
      "Epoch 91, Loss: 9090.1065\n",
      "Epoch 92, Loss: 9090.1277\n",
      "Epoch 93, Loss: 9090.0424\n",
      "Epoch 94, Loss: 9090.0662\n",
      "Epoch 95, Loss: 9090.1499\n",
      "Epoch 96, Loss: 9089.9384\n",
      "Epoch 97, Loss: 9089.9734\n",
      "Epoch 98, Loss: 9089.9559\n",
      "Epoch 99, Loss: 9089.8608\n",
      "Epoch 100, Loss: 9089.9026\n",
      "Epoch 101, Loss: 9089.8386\n",
      "Epoch 102, Loss: 9089.8284\n",
      "Epoch 103, Loss: 9089.8427\n",
      "Epoch 104, Loss: 9089.8962\n",
      "Epoch 105, Loss: 9089.9014\n",
      "Epoch 106, Loss: 9089.7303\n",
      "Epoch 107, Loss: 9089.7345\n",
      "Epoch 108, Loss: 9089.7479\n",
      "Epoch 109, Loss: 9089.7104\n",
      "Epoch 110, Loss: 9089.6751\n",
      "Epoch 111, Loss: 9089.6822\n",
      "Epoch 112, Loss: 9089.7974\n",
      "Epoch 113, Loss: 9089.6750\n",
      "Epoch 114, Loss: 9089.6540\n",
      "Epoch 115, Loss: 9089.7051\n",
      "Epoch 116, Loss: 9089.6556\n",
      "Epoch 117, Loss: 9089.6041\n",
      "Epoch 118, Loss: 9089.6260\n",
      "Epoch 119, Loss: 9089.5893\n",
      "Epoch 120, Loss: 9089.5232\n",
      "Epoch 121, Loss: 9089.5639\n",
      "Epoch 122, Loss: 9089.5528\n",
      "Epoch 123, Loss: 9089.5009\n",
      "Epoch 124, Loss: 9089.4888\n",
      "Epoch 125, Loss: 9089.4475\n",
      "Epoch 126, Loss: 9089.4384\n",
      "Epoch 127, Loss: 9089.4973\n",
      "Epoch 128, Loss: 9089.3902\n",
      "Epoch 129, Loss: 9089.4631\n",
      "Epoch 130, Loss: 9089.4290\n",
      "Epoch 131, Loss: 9089.3859\n",
      "Epoch 132, Loss: 9089.4658\n",
      "Epoch 133, Loss: 9089.4527\n",
      "Epoch 134, Loss: 9089.3449\n",
      "Epoch 135, Loss: 9089.3474\n",
      "Epoch 136, Loss: 9089.2515\n",
      "Epoch 137, Loss: 9089.3576\n",
      "Epoch 138, Loss: 9089.2981\n",
      "Epoch 139, Loss: 9089.2796\n",
      "Epoch 140, Loss: 9089.3128\n",
      "Epoch 141, Loss: 9089.2298\n",
      "Epoch 142, Loss: 9089.3576\n",
      "Epoch 143, Loss: 9089.2372\n",
      "Epoch 144, Loss: 9089.2423\n",
      "Epoch 145, Loss: 9089.2094\n",
      "Epoch 146, Loss: 9089.1874\n",
      "Epoch 147, Loss: 9089.1413\n",
      "Epoch 148, Loss: 9089.2930\n",
      "Epoch 149, Loss: 9089.1299\n",
      "Epoch 150, Loss: 9089.1587\n",
      "Epoch 151, Loss: 9089.1085\n",
      "Epoch 152, Loss: 9089.0859\n",
      "Epoch 153, Loss: 9089.1680\n",
      "Epoch 154, Loss: 9089.2135\n",
      "Epoch 155, Loss: 9089.1090\n",
      "Epoch 156, Loss: 9089.1311\n",
      "Epoch 157, Loss: 9089.0604\n",
      "Epoch 158, Loss: 9089.0826\n",
      "Epoch 159, Loss: 9089.0576\n",
      "Epoch 160, Loss: 9089.1051\n",
      "Epoch 161, Loss: 9088.9942\n",
      "Epoch 162, Loss: 9088.9751\n",
      "Epoch 163, Loss: 9089.0205\n",
      "Epoch 164, Loss: 9089.0471\n",
      "Epoch 165, Loss: 9089.0034\n",
      "Epoch 166, Loss: 9088.9580\n",
      "Epoch 167, Loss: 9089.0334\n",
      "Epoch 168, Loss: 9088.9921\n",
      "Epoch 169, Loss: 9088.9834\n",
      "Epoch 170, Loss: 9088.9995\n",
      "Epoch 171, Loss: 9088.9475\n",
      "Epoch 172, Loss: 9088.8802\n",
      "Epoch 173, Loss: 9088.9427\n",
      "Epoch 174, Loss: 9088.9593\n",
      "Epoch 175, Loss: 9088.9500\n",
      "Epoch 176, Loss: 9088.9270\n",
      "Epoch 177, Loss: 9088.9326\n",
      "Epoch 178, Loss: 9088.8433\n",
      "Epoch 179, Loss: 9088.8675\n",
      "Epoch 180, Loss: 9088.8219\n",
      "Epoch 181, Loss: 9088.9060\n",
      "Epoch 182, Loss: 9088.8243\n",
      "Epoch 183, Loss: 9088.9596\n",
      "Epoch 184, Loss: 9088.8562\n",
      "Epoch 185, Loss: 9088.9174\n",
      "Epoch 186, Loss: 9088.8072\n",
      "Epoch 187, Loss: 9088.9243\n",
      "Epoch 188, Loss: 9088.8495\n",
      "Epoch 189, Loss: 9088.8345\n",
      "Epoch 190, Loss: 9088.7902\n",
      "Epoch 191, Loss: 9088.7463\n",
      "Epoch 192, Loss: 9088.7478\n",
      "Epoch 193, Loss: 9088.7701\n",
      "Epoch 194, Loss: 9088.7772\n",
      "Epoch 195, Loss: 9088.7625\n",
      "Epoch 196, Loss: 9088.7558\n",
      "Epoch 197, Loss: 9088.6854\n",
      "Epoch 198, Loss: 9088.7651\n",
      "Epoch 199, Loss: 9088.7104\n",
      "Epoch 200, Loss: 9088.6912\n",
      "Epoch 201, Loss: 9088.6803\n",
      "Epoch 202, Loss: 9088.6566\n",
      "Epoch 203, Loss: 9088.6088\n",
      "Epoch 204, Loss: 9088.6541\n",
      "Epoch 205, Loss: 9088.7382\n",
      "Epoch 206, Loss: 9088.6976\n",
      "Epoch 207, Loss: 9088.6625\n",
      "Epoch 208, Loss: 9088.6829\n",
      "Epoch 209, Loss: 9088.6483\n",
      "Epoch 210, Loss: 9088.6662\n",
      "Epoch 211, Loss: 9088.6217\n",
      "Epoch 212, Loss: 9088.6442\n",
      "Epoch 213, Loss: 9088.6342\n",
      "Epoch 214, Loss: 9088.6593\n",
      "Epoch 215, Loss: 9088.5522\n",
      "Epoch 216, Loss: 9088.6596\n",
      "Epoch 217, Loss: 9088.5963\n",
      "Epoch 218, Loss: 9088.6595\n",
      "Epoch 219, Loss: 9088.6653\n",
      "Epoch 220, Loss: 9088.5324\n",
      "Epoch 221, Loss: 9088.5134\n",
      "Epoch 222, Loss: 9088.6169\n",
      "Epoch 223, Loss: 9088.6165\n",
      "Epoch 224, Loss: 9088.5950\n",
      "Epoch 225, Loss: 9088.6317\n",
      "Epoch 226, Loss: 9088.5655\n",
      "Epoch 227, Loss: 9088.4635\n",
      "Epoch 228, Loss: 9088.5383\n",
      "Epoch 229, Loss: 9088.4996\n",
      "Epoch 230, Loss: 9088.5387\n",
      "Epoch 231, Loss: 9088.5540\n",
      "Epoch 232, Loss: 9088.5192\n",
      "Epoch 233, Loss: 9088.5292\n",
      "Epoch 234, Loss: 9088.5320\n",
      "Epoch 235, Loss: 9088.4718\n",
      "Epoch 236, Loss: 9088.5274\n",
      "Epoch 237, Loss: 9088.4620\n",
      "Epoch 238, Loss: 9088.5093\n",
      "Epoch 239, Loss: 9088.4394\n",
      "Epoch 240, Loss: 9088.4918\n",
      "Epoch 241, Loss: 9088.4721\n",
      "Epoch 242, Loss: 9088.5308\n",
      "Epoch 243, Loss: 9088.4692\n",
      "Epoch 244, Loss: 9088.4468\n",
      "Epoch 245, Loss: 9088.4471\n",
      "Epoch 246, Loss: 9088.4059\n",
      "Epoch 247, Loss: 9088.3999\n",
      "Epoch 248, Loss: 9088.4017\n",
      "Epoch 249, Loss: 9088.4048\n",
      "Epoch 250, Loss: 9088.5289\n",
      "Epoch 251, Loss: 9088.4321\n",
      "Epoch 252, Loss: 9088.4714\n",
      "Epoch 253, Loss: 9088.4282\n",
      "Epoch 254, Loss: 9088.4969\n",
      "Epoch 255, Loss: 9088.3355\n",
      "Epoch 256, Loss: 9088.4448\n",
      "Epoch 257, Loss: 9088.4010\n",
      "Epoch 258, Loss: 9088.3968\n",
      "Epoch 259, Loss: 9088.3905\n",
      "Epoch 260, Loss: 9088.3376\n",
      "Epoch 261, Loss: 9088.3370\n",
      "Epoch 262, Loss: 9088.3835\n",
      "Epoch 263, Loss: 9088.3167\n",
      "Epoch 264, Loss: 9088.3942\n",
      "Epoch 265, Loss: 9088.3110\n",
      "Epoch 266, Loss: 9088.3552\n",
      "Epoch 267, Loss: 9088.4654\n",
      "Epoch 268, Loss: 9088.3172\n",
      "Epoch 269, Loss: 9088.3238\n",
      "Epoch 270, Loss: 9088.3560\n",
      "Epoch 271, Loss: 9088.3517\n",
      "Epoch 272, Loss: 9088.3299\n",
      "Epoch 273, Loss: 9088.3013\n",
      "Epoch 274, Loss: 9088.2804\n",
      "Epoch 275, Loss: 9088.2752\n",
      "Epoch 276, Loss: 9088.3536\n",
      "Epoch 277, Loss: 9088.2879\n",
      "Epoch 278, Loss: 9088.3173\n",
      "Epoch 279, Loss: 9088.3279\n",
      "Epoch 280, Loss: 9088.2843\n",
      "Epoch 281, Loss: 9088.2173\n",
      "Epoch 282, Loss: 9088.2717\n",
      "Epoch 283, Loss: 9088.2294\n",
      "Epoch 284, Loss: 9088.2961\n",
      "Epoch 285, Loss: 9088.3349\n",
      "Epoch 286, Loss: 9088.2446\n",
      "Epoch 287, Loss: 9088.3022\n",
      "Epoch 288, Loss: 9088.3005\n",
      "Epoch 289, Loss: 9088.2361\n",
      "Epoch 290, Loss: 9088.2731\n",
      "Epoch 291, Loss: 9088.2080\n",
      "Epoch 292, Loss: 9088.2670\n",
      "Epoch 293, Loss: 9088.2738\n",
      "Epoch 294, Loss: 9088.2838\n",
      "Epoch 295, Loss: 9088.2187\n",
      "Epoch 296, Loss: 9088.2369\n",
      "Epoch 297, Loss: 9088.2001\n",
      "Epoch 298, Loss: 9088.2498\n",
      "Epoch 299, Loss: 9088.2262\n",
      "Epoch 300, Loss: 9088.1619\n",
      "Epoch 301, Loss: 9088.2491\n",
      "Epoch 302, Loss: 9088.1920\n",
      "Epoch 303, Loss: 9088.2257\n",
      "Epoch 304, Loss: 9088.1971\n",
      "Epoch 305, Loss: 9088.1877\n",
      "Epoch 306, Loss: 9088.1762\n",
      "Epoch 307, Loss: 9088.2283\n",
      "Epoch 308, Loss: 9088.2556\n",
      "Epoch 309, Loss: 9088.1861\n",
      "Epoch 310, Loss: 9088.2422\n",
      "Epoch 311, Loss: 9088.1130\n",
      "Epoch 312, Loss: 9088.1639\n",
      "Epoch 313, Loss: 9088.1382\n",
      "Epoch 314, Loss: 9088.1512\n",
      "Epoch 315, Loss: 9088.1387\n",
      "Epoch 316, Loss: 9088.1217\n",
      "Epoch 317, Loss: 9088.1357\n",
      "Epoch 318, Loss: 9088.2099\n",
      "Epoch 319, Loss: 9088.1157\n",
      "Epoch 320, Loss: 9088.1437\n",
      "Epoch 321, Loss: 9088.0874\n",
      "Epoch 322, Loss: 9088.1751\n",
      "Epoch 323, Loss: 9088.0753\n",
      "Epoch 324, Loss: 9088.1147\n",
      "Epoch 325, Loss: 9088.1375\n",
      "Epoch 326, Loss: 9088.1077\n",
      "Epoch 327, Loss: 9088.0575\n",
      "Epoch 328, Loss: 9088.0799\n",
      "Epoch 329, Loss: 9088.0261\n",
      "Epoch 330, Loss: 9088.1514\n",
      "Epoch 331, Loss: 9088.0573\n",
      "Epoch 332, Loss: 9088.0760\n",
      "Epoch 333, Loss: 9088.1067\n",
      "Epoch 334, Loss: 9088.0955\n",
      "Epoch 335, Loss: 9088.0616\n",
      "Epoch 336, Loss: 9088.0651\n",
      "Epoch 337, Loss: 9088.0363\n",
      "Epoch 338, Loss: 9088.0676\n",
      "Epoch 339, Loss: 9088.0935\n",
      "Epoch 340, Loss: 9088.0752\n",
      "Epoch 341, Loss: 9087.9966\n",
      "Epoch 342, Loss: 9088.0758\n",
      "Epoch 343, Loss: 9088.0336\n",
      "Epoch 344, Loss: 9088.0416\n",
      "Epoch 345, Loss: 9088.0661\n",
      "Epoch 346, Loss: 9088.0187\n",
      "Epoch 347, Loss: 9087.9681\n",
      "Epoch 348, Loss: 9088.0375\n",
      "Epoch 349, Loss: 9088.0610\n",
      "Epoch 350, Loss: 9087.9818\n",
      "Epoch 351, Loss: 9088.0247\n",
      "Epoch 352, Loss: 9088.0467\n",
      "Epoch 353, Loss: 9087.9696\n",
      "Epoch 354, Loss: 9088.0537\n",
      "Epoch 355, Loss: 9088.0632\n",
      "Epoch 356, Loss: 9088.0242\n",
      "Epoch 357, Loss: 9088.1043\n",
      "Epoch 358, Loss: 9088.0013\n",
      "Epoch 359, Loss: 9087.9952\n",
      "Epoch 360, Loss: 9087.9812\n",
      "Epoch 361, Loss: 9088.0109\n",
      "Epoch 362, Loss: 9087.9307\n",
      "Epoch 363, Loss: 9087.9738\n",
      "Epoch 364, Loss: 9087.9959\n",
      "Epoch 365, Loss: 9088.0466\n",
      "Epoch 366, Loss: 9088.0106\n",
      "Epoch 367, Loss: 9088.0113\n",
      "Epoch 368, Loss: 9087.9051\n",
      "Epoch 369, Loss: 9088.0254\n",
      "Epoch 370, Loss: 9088.0074\n",
      "Epoch 371, Loss: 9088.0269\n",
      "Epoch 372, Loss: 9087.8647\n",
      "Epoch 373, Loss: 9087.9517\n",
      "Epoch 374, Loss: 9087.9108\n",
      "Epoch 375, Loss: 9087.9175\n",
      "Epoch 376, Loss: 9087.9136\n",
      "Epoch 377, Loss: 9088.0219\n",
      "Epoch 378, Loss: 9087.9957\n",
      "Epoch 379, Loss: 9087.9117\n",
      "Epoch 380, Loss: 9087.8776\n",
      "Epoch 381, Loss: 9087.9393\n",
      "Epoch 382, Loss: 9087.9775\n",
      "Epoch 383, Loss: 9087.9174\n",
      "Epoch 384, Loss: 9088.0277\n",
      "Epoch 385, Loss: 9087.9177\n",
      "Epoch 386, Loss: 9087.9534\n",
      "Epoch 387, Loss: 9087.9634\n",
      "Epoch 388, Loss: 9087.8881\n",
      "Epoch 389, Loss: 9087.9123\n",
      "Epoch 390, Loss: 9087.8810\n",
      "Epoch 391, Loss: 9087.8889\n",
      "Epoch 392, Loss: 9087.8639\n",
      "Epoch 393, Loss: 9087.9573\n",
      "Epoch 394, Loss: 9087.8676\n",
      "Epoch 395, Loss: 9087.9419\n",
      "Epoch 396, Loss: 9087.8487\n",
      "Epoch 397, Loss: 9087.9246\n",
      "Epoch 398, Loss: 9087.9132\n",
      "Epoch 399, Loss: 9087.9196\n",
      "Epoch 400, Loss: 9087.8805\n",
      "Epoch 401, Loss: 9087.8560\n",
      "Epoch 402, Loss: 9087.8574\n",
      "Epoch 403, Loss: 9087.8348\n",
      "Epoch 404, Loss: 9087.8044\n",
      "Epoch 405, Loss: 9087.9011\n",
      "Epoch 406, Loss: 9087.8102\n",
      "Epoch 407, Loss: 9087.9020\n",
      "Epoch 408, Loss: 9087.9359\n",
      "Epoch 409, Loss: 9087.8539\n",
      "Epoch 410, Loss: 9087.8567\n",
      "Epoch 411, Loss: 9087.8265\n",
      "Epoch 412, Loss: 9087.8625\n",
      "Epoch 413, Loss: 9087.8422\n",
      "Epoch 414, Loss: 9087.8355\n",
      "Epoch 415, Loss: 9087.8906\n",
      "Epoch 416, Loss: 9087.8036\n",
      "Epoch 417, Loss: 9087.8813\n",
      "Epoch 418, Loss: 9087.8262\n",
      "Epoch 419, Loss: 9087.9015\n",
      "Epoch 420, Loss: 9087.9056\n",
      "Epoch 421, Loss: 9087.8621\n",
      "Epoch 422, Loss: 9087.8093\n",
      "Epoch 423, Loss: 9087.8292\n",
      "Epoch 424, Loss: 9087.7898\n",
      "Epoch 425, Loss: 9087.8296\n",
      "Epoch 426, Loss: 9087.8439\n",
      "Epoch 427, Loss: 9087.8108\n",
      "Epoch 428, Loss: 9087.7832\n",
      "Epoch 429, Loss: 9087.7971\n",
      "Epoch 430, Loss: 9087.7806\n",
      "Epoch 431, Loss: 9087.7488\n",
      "Epoch 432, Loss: 9087.8653\n",
      "Epoch 433, Loss: 9087.7736\n",
      "Epoch 434, Loss: 9087.7910\n",
      "Epoch 435, Loss: 9087.7933\n",
      "Epoch 436, Loss: 9087.8308\n",
      "Epoch 437, Loss: 9087.8258\n",
      "Epoch 438, Loss: 9087.8307\n",
      "Epoch 439, Loss: 9087.7724\n",
      "Epoch 440, Loss: 9087.7857\n",
      "Epoch 441, Loss: 9087.8700\n",
      "Epoch 442, Loss: 9087.7776\n",
      "Epoch 443, Loss: 9087.8402\n",
      "Epoch 444, Loss: 9087.8280\n",
      "Epoch 445, Loss: 9087.7505\n",
      "Epoch 446, Loss: 9087.7851\n",
      "Epoch 447, Loss: 9087.7435\n",
      "Epoch 448, Loss: 9087.7170\n",
      "Epoch 449, Loss: 9087.8254\n",
      "Epoch 450, Loss: 9087.7982\n",
      "Epoch 451, Loss: 9087.7479\n",
      "Epoch 452, Loss: 9087.7148\n",
      "Epoch 453, Loss: 9087.7965\n",
      "Epoch 454, Loss: 9087.6868\n",
      "Epoch 455, Loss: 9087.7856\n",
      "Epoch 456, Loss: 9087.7548\n",
      "Epoch 457, Loss: 9087.8065\n",
      "Epoch 458, Loss: 9087.8000\n",
      "Epoch 459, Loss: 9087.7361\n",
      "Epoch 460, Loss: 9087.7389\n",
      "Epoch 461, Loss: 9087.6978\n",
      "Epoch 462, Loss: 9087.7773\n",
      "Epoch 463, Loss: 9087.7646\n",
      "Epoch 464, Loss: 9087.7108\n",
      "Epoch 465, Loss: 9087.7419\n",
      "Epoch 466, Loss: 9087.7502\n",
      "Epoch 467, Loss: 9087.7214\n",
      "Epoch 468, Loss: 9087.8018\n",
      "Epoch 469, Loss: 9087.7450\n",
      "Epoch 470, Loss: 9087.7572\n",
      "Epoch 471, Loss: 9087.7099\n",
      "Epoch 472, Loss: 9087.7485\n",
      "Epoch 473, Loss: 9087.7250\n",
      "Epoch 474, Loss: 9087.7406\n",
      "Epoch 475, Loss: 9087.7117\n",
      "Epoch 476, Loss: 9087.7721\n",
      "Epoch 477, Loss: 9087.7222\n",
      "Epoch 478, Loss: 9087.6914\n",
      "Epoch 479, Loss: 9087.7097\n",
      "Epoch 480, Loss: 9087.6924\n",
      "Epoch 481, Loss: 9087.7313\n",
      "Epoch 482, Loss: 9087.7929\n",
      "Epoch 483, Loss: 9087.6864\n",
      "Epoch 484, Loss: 9087.6947\n",
      "Epoch 485, Loss: 9087.7436\n",
      "Epoch 486, Loss: 9087.6852\n",
      "Epoch 487, Loss: 9087.6986\n",
      "Epoch 488, Loss: 9087.8128\n",
      "Epoch 489, Loss: 9087.7196\n",
      "Epoch 490, Loss: 9087.7243\n",
      "Epoch 491, Loss: 9087.6946\n",
      "Epoch 492, Loss: 9087.7371\n",
      "Epoch 493, Loss: 9087.6606\n",
      "Epoch 494, Loss: 9087.7412\n",
      "Epoch 495, Loss: 9087.6935\n",
      "Epoch 496, Loss: 9087.6768\n",
      "Epoch 497, Loss: 9087.6522\n",
      "Epoch 498, Loss: 9087.7164\n",
      "Epoch 499, Loss: 9087.7129\n",
      "Epoch 500, Loss: 9087.6476\n"
     ]
    }
   ],
   "source": [
    "intersection_data_df = intersection_data['exprs_intersect.csv']\n",
    "X = torch.FloatTensor(intersection_data_df.values)\n",
    "patient_ids = intersection_data_df.index\n",
    "# Normalisieren \n",
    "X_normalized = (X - X.min()) / (X.max() - X.min())\n",
    "X_normalized = X_normalized.float()\n",
    "# Define VAE architecture\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(128, latent_dim)\n",
    "        self.fc_var = nn.Linear(128, latent_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_var(h)\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return self.decode(z), mu, log_var\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = X.shape[1]\n",
    "latent_dim = 512\n",
    "batch_size = 128\n",
    "epochs = 500 \n",
    "learning_rate = 1e-5\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = VAE(input_dim, latent_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Prepare data loader\n",
    "dataset = TensorDataset(X_normalized)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data,) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, log_var = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, log_var)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {train_loss / len(dataloader.dataset):.4f}')\n",
    "\n",
    "# Function to get latent space representation\n",
    "def get_latent_representation(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        mu, _ = model.encode(data)\n",
    "    return mu\n",
    "\n",
    "# Get latent representation of your data\n",
    "latent_repr = get_latent_representation(model, X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nach der Erzeugung der latenten Repräsentation\n",
    "latent_np = latent_repr.numpy()\n",
    "latent_df = pd.DataFrame(latent_np, index=patient_ids, columns=[f'latent_{i}' for i in range(latent_np.shape[1])])\n",
    "# Angenommen, Ihre Überlebensdaten sind in einem DataFrame namens 'survival_data'\n",
    "survival_data=merged_pdata_imputed['merged_imputed_pData.csv']\n",
    "# Angenommen, Ihre Überlebensdaten sind in einem DataFrame namens 'survival_data'\n",
    "merged_data = pd.concat([latent_df, survival_data[['MONTH_TO_BCR', 'BCR_STATUS']]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten vorbereiten\n",
    "X = merged_data.drop(['MONTH_TO_BCR', 'BCR_STATUS'], axis=1)\n",
    "y = np.array([(status, time) for status, time in zip(merged_data['BCR_STATUS'], merged_data['MONTH_TO_BCR'])],\n",
    "             dtype=[('status', bool), ('time', float)])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE + Random Survival Forest C-index: 0.6742\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Laden der Daten\n",
    "exprs = loader.common_genes_data['common_genes_knn_imputed.csv']\n",
    "survival_data = merged_pdata_imputed['merged_imputed_pData.csv']\n",
    "\n",
    "# Funktion zum Vorbereiten der Daten\n",
    "def prepare_data(X, survival_data):\n",
    "    merged_data = pd.concat([X, survival_data[['MONTH_TO_BCR', 'BCR_STATUS']]], axis=1)\n",
    "    X = merged_data.drop(['MONTH_TO_BCR', 'BCR_STATUS'], axis=1)\n",
    "    y = np.array([(status, time) for status, time in zip(merged_data['BCR_STATUS'], merged_data['MONTH_TO_BCR'])],\n",
    "                 dtype=[('status', bool), ('time', float)])\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Funktion zum Fitten und Evaluieren des RSF\n",
    "def fit_rsf(X_train, y_train, X_test, y_test):\n",
    "    rsf = RandomSurvivalForest(n_estimators=100, random_state=42)\n",
    "    rsf.fit(X_train, y_train)\n",
    "    predictions = rsf.predict(X_test)\n",
    "    c_index = concordance_index_censored(y_test['status'], y_test['time'], predictions)[0]\n",
    "    return c_index\n",
    "\n",
    "# 1. RSF auf Basis der VAE latenten Repräsentation\n",
    "# (Ihr bestehender Code für VAE)\n",
    "latent_np = latent_repr.numpy()\n",
    "latent_df = pd.DataFrame(latent_np, index=patient_ids, columns=[f'latent_{i}' for i in range(latent_np.shape[1])])\n",
    "\n",
    "X_train_vae, X_test_vae, y_train_vae, y_test_vae = prepare_data(latent_df, survival_data)\n",
    "c_index_rsf_vae = fit_rsf(X_train_vae, y_train_vae, X_test_vae, y_test_vae)\n",
    "print(f\"VAE + Random Survival Forest C-index: {c_index_rsf_vae:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA + Random Survival Forest C-index: 0.6671\n"
     ]
    }
   ],
   "source": [
    "# 2. RSF auf Basis der PCA\n",
    "\n",
    "X_train_pca, X_test_pca, y_train_pca, y_test_pca = prepare_data(pca_df, survival_data)\n",
    "c_index_rsf_pca = fit_rsf(X_train_pca, y_train_pca, X_test_pca, y_test_pca)\n",
    "print(f\"PCA + Random Survival Forest C-index: {c_index_rsf_pca:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE + Random Survival Forest C-index: 0.5050\n",
      "PCA + Random Survival Forest C-index: 0.6312\n",
      "Original Gene Data + Random Survival Forest C-index: 0.6802\n",
      "VAE + RSF: C-index = 0.5050\n",
      "PCA + RSF: C-index = 0.6312\n",
      "Original + RSF: C-index = 0.6802\n"
     ]
    }
   ],
   "source": [
    "# 3. RSF auf Basis der originalen Gendaten ohne Dimensionsreduktion\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = prepare_data(exprs, survival_data)\n",
    "c_index_rsf_orig = fit_rsf(X_train_orig, y_train_orig, X_test_orig, y_test_orig)\n",
    "print(f\"Original Gene Data + Random Survival Forest C-index: {c_index_rsf_orig:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deep Survival Neural Net\n",
    "->Performed extrem schlecht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Laden der Daten\n",
    "exprs = loader.intersection_data['exprs_intersect.csv']\n",
    "survival_data = merged_pdata_imputed['merged_imputed_pData.csv']\n",
    "\n",
    "# Funktion zum Vorbereiten der Daten\n",
    "def prepare_data(X, survival_data):\n",
    "    merged_data = pd.concat([X, survival_data[['MONTH_TO_BCR', 'BCR_STATUS']]], axis=1)\n",
    "    X = merged_data.drop(['MONTH_TO_BCR', 'BCR_STATUS'], axis=1)\n",
    "    y = np.array([(status, time) for status, time in zip(merged_data['BCR_STATUS'], merged_data['MONTH_TO_BCR'])],\n",
    "                 dtype=[('status', bool), ('time', float)])\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "class DeepSurv(nn.Module):\n",
    "    def __init__(self, in_features, hidden_layers=[64, 32, 16]):\n",
    "        super(DeepSurv, self).__init__()\n",
    "        layers = []\n",
    "        prev_neurons = in_features\n",
    "        for neurons in hidden_layers:\n",
    "            layers.append(nn.Linear(prev_neurons, neurons))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.BatchNorm1d(neurons))\n",
    "            layers.append(nn.Dropout(0.3))\n",
    "            prev_neurons = neurons\n",
    "        layers.append(nn.Linear(prev_neurons, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def negative_log_likelihood(risk_scores, y_time, y_status):\n",
    "    hazard_ratio = torch.exp(risk_scores)\n",
    "    log_risk = torch.log(torch.cumsum(hazard_ratio, dim=0))\n",
    "    uncensored_likelihood = risk_scores - log_risk\n",
    "    censored_likelihood = uncensored_likelihood * y_status\n",
    "    neg_likelihood = -torch.sum(censored_likelihood)\n",
    "    return neg_likelihood\n",
    "\n",
    "def fit_deep_surv(X_train, y_train, X_test, y_test, epochs=1000, lr=0.001, batch_size=64):\n",
    "    model = DeepSurv(X_train.shape[1])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "    \n",
    "    X_train_tensor = torch.FloatTensor(X_train.values)\n",
    "    y_train_time = torch.FloatTensor(y_train['time'].astype(float))\n",
    "    y_train_status = torch.FloatTensor(y_train['status'].astype(float))\n",
    "    \n",
    "    n_batches = int(np.ceil(len(X_train) / batch_size))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for i in range(n_batches):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            x_batch = X_train_tensor[start:end]\n",
    "            y_time_batch = y_train_time[start:end]\n",
    "            y_status_batch = y_train_status[start:end]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            risk_scores = model(x_batch).squeeze()\n",
    "            loss = negative_log_likelihood(risk_scores, y_time_batch, y_status_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / n_batches\n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        risk_scores = model(torch.FloatTensor(X_test.values)).squeeze().numpy()\n",
    "    \n",
    "    c_index = concordance_index_censored(y_test['status'], y_test['time'], -risk_scores)[0]\n",
    "    return model, c_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonasschernich/Library/Mobile Documents/com~apple~CloudDocs/Finanzen/Lev Strategy/StatisticalConsulting/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000, Loss: 18.0329\n",
      "Epoch 200/1000, Loss: 17.6683\n",
      "Epoch 300/1000, Loss: 18.2139\n",
      "Epoch 400/1000, Loss: 17.8029\n",
      "Epoch 500/1000, Loss: 17.9357\n",
      "Epoch 600/1000, Loss: 17.9638\n",
      "Epoch 700/1000, Loss: 17.2288\n",
      "Epoch 800/1000, Loss: 17.5546\n",
      "Epoch 900/1000, Loss: 18.5437\n",
      "Epoch 1000/1000, Loss: 16.6660\n",
      "VAE + Deep Survival Neural Network C-index: 0.3755\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. VAE-based DeepSurv\n",
    "X_train_vae, X_test_vae, y_train_vae, y_test_vae = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "deep_surv_model_vae, c_index_deep_surv_vae = fit_deep_surv(X_train_vae, y_train_vae, X_test_vae, y_test_vae)\n",
    "print(f\"VAE + Deep Survival Neural Network C-index: {c_index_deep_surv_vae:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonasschernich/Library/Mobile Documents/com~apple~CloudDocs/Finanzen/Lev Strategy/StatisticalConsulting/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000, Loss: 18.8130\n",
      "Epoch 200/1000, Loss: 18.0942\n",
      "Epoch 300/1000, Loss: 17.0494\n",
      "Epoch 400/1000, Loss: 18.4759\n",
      "Epoch 500/1000, Loss: 17.7021\n",
      "Epoch 600/1000, Loss: 17.1172\n",
      "Epoch 700/1000, Loss: 18.6699\n",
      "Epoch 800/1000, Loss: 17.1137\n",
      "Epoch 900/1000, Loss: 18.2977\n",
      "Epoch 1000/1000, Loss: 18.1197\n",
      "PCA + Deep Survival Neural Network C-index: 0.4127\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 2. PCA-based DeepSurv\n",
    "X_train_pca, X_test_pca, y_train_pca, y_test_pca = prepare_data(pca_df, survival_data)\n",
    "deep_surv_model_pca, c_index_deep_surv_pca = fit_deep_surv(X_train_pca, y_train_pca, X_test_pca, y_test_pca)\n",
    "print(f\"PCA + Deep Survival Neural Network C-index: {c_index_deep_surv_pca:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonasschernich/Library/Mobile Documents/com~apple~CloudDocs/Finanzen/Lev Strategy/StatisticalConsulting/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000, Loss: 18.4014\n",
      "Epoch 200/1000, Loss: 18.3054\n",
      "Epoch 300/1000, Loss: 19.1651\n",
      "Epoch 400/1000, Loss: 18.5330\n",
      "Epoch 500/1000, Loss: 18.5621\n",
      "Epoch 600/1000, Loss: 18.5519\n",
      "Epoch 700/1000, Loss: 19.1253\n",
      "Epoch 800/1000, Loss: 18.9325\n",
      "Epoch 900/1000, Loss: 19.3670\n",
      "Epoch 1000/1000, Loss: 19.9155\n",
      "Original Gene Data + Deep Survival Neural Network C-index: 0.3581\n"
     ]
    }
   ],
   "source": [
    "# 3. Original gene data-based DeepSurv\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = prepare_data(exprs, survival_data)\n",
    "deep_surv_model_orig, c_index_deep_surv_orig = fit_deep_surv(X_train_orig, y_train_orig, X_test_orig, y_test_orig)\n",
    "print(f\"Original Gene Data + Deep Survival Neural Network C-index: {c_index_deep_surv_orig:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
