{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T06:42:50.573110Z",
     "start_time": "2024-11-08T06:41:46.060265Z"
    }
   },
   "source": [
    "\n",
    "from preprocessing import DataLoader\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "base_path = os.path.dirname(os.getcwd()) \n",
    "loader = DataLoader(base_path)\n",
    "loader.load_all_data()"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T06:42:54.019169Z",
     "start_time": "2024-11-08T06:42:54.012071Z"
    }
   },
   "source": [
    "# Expressions-Daten\n",
    "exprs_data = loader.exprs_data\n",
    "\n",
    "# Originale pData\n",
    "pdata_original = loader.pdata_original\n",
    "\n",
    "# Imputierte pData\n",
    "pdata_imputed = loader.pdata_imputed\n",
    "\n",
    "# All Genes Daten\n",
    "all_genes_data = loader.all_genes_data\n",
    "\n",
    "# Common Genes Daten\n",
    "common_genes_data = loader.common_genes_data\n",
    "\n",
    "# Intersection Daten\n",
    "intersection_data = loader.intersection_data\n",
    "\n",
    "# Merged originale pData\n",
    "merged_pdata_original = loader.merged_pdata_original\n",
    "\n",
    "# Merged imputierte pData\n",
    "merged_pdata_imputed = loader.merged_pdata_imputed"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "all_genes_data['all_genes.csv'].shape[1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T06:42:56.446911Z",
     "start_time": "2024-11-08T06:42:56.437859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "exprs = loader.common_genes_data['common_genes_knn_imputed.csv']\n",
    "pdata = loader.merged_pdata_imputed['merged_imputed_pData.csv']\n",
    "clinical_features = [\n",
    "    'GLEASON_SCORE',\n",
    "    'PATH_T_STAGE', 'CLIN_T_STAGE',\n",
    "    'PRE_OPERATIVE_PSA', 'AGE'\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T06:29:44.806377Z",
     "start_time": "2024-11-08T06:29:42.236021Z"
    }
   },
   "source": [
    "from preprocessing import DataLoader\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "#from sksurv.linear_model import CoxnetSurvivalAnalysis\n",
    "#from sksurv.util import Surv\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import matplotlib.pyplot as plt\n",
    "#from sksurv.ensemble import RandomSurvivalForest, GradientBoostingSurvivalAnalysis, ComponentwiseGradientBoostingSurvivalAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "import xgboost as xgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from lifelines.utils import concordance_index\n",
    "\n"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sksurv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 18\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m#from sksurv.ensemble import RandomSurvivalForest, GradientBoostingSurvivalAnalysis, ComponentwiseGradientBoostingSurvivalAnalysis\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mensemble\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RandomForestClassifier\n\u001B[0;32m---> 18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msksurv\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m concordance_index_censored\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mxgboost\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mxgb\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'sksurv'"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T06:43:08.320469Z",
     "start_time": "2024-11-08T06:43:00.439949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from lifelines.utils import concordance_index\n",
    "import xgboost as xgb\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def prepare_data(data, categorical_features, numeric_features):\n",
    "    \"\"\"Prepare data by encoding categorical variables and scaling numeric ones\"\"\"\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_features),\n",
    "            ('cat', OneHotEncoder(drop='first', sparse=False), categorical_features)\n",
    "        ])\n",
    "    \n",
    "    X_transformed = preprocessor.fit_transform(data)\n",
    "    \n",
    "    # Get feature names after transformation\n",
    "    cat_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
    "    all_feature_names = numeric_features.tolist() + cat_feature_names.tolist()\n",
    "    \n",
    "    return X_transformed, preprocessor, all_feature_names\n",
    "\n",
    "class SimpleDeepSurv(nn.Module):\n",
    "    def __init__(self, in_features, hidden_size=32):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def train_deepsurv(X, time, event, epochs=10, lr=0.01):\n",
    "    X_tensor = torch.FloatTensor(X)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = SimpleDeepSurv(X.shape[1])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        risk_scores = model(X_tensor)\n",
    "        \n",
    "        # Calculate negative log likelihood loss\n",
    "        log_risk = risk_scores\n",
    "        censored_likelihood = log_risk.squeeze()\n",
    "        uncensored_likelihood = log_risk.squeeze()\n",
    "        \n",
    "        # Cox loss\n",
    "        loss = -torch.sum(uncensored_likelihood[event == 1] - \n",
    "                         torch.log(torch.sum(torch.exp(log_risk)[time >= time[event == 1][:, None]])))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def predict_deepsurv(model, X):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        return -model(torch.FloatTensor(X)).numpy().flatten()\n",
    "\n",
    "class SimpleRSF:\n",
    "    def __init__(self, n_estimators=100):\n",
    "        self.model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            min_samples_split=10,\n",
    "            min_samples_leaf=5,\n",
    "            max_features='sqrt',\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "    def fit(self, X, time, event):\n",
    "        self.model.fit(X, -time)\n",
    "        return self\n",
    "        \n",
    "    def predict_risk(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "class SimpleGBSurvival:\n",
    "    def __init__(self, n_estimators=100):\n",
    "        self.model = xgb.XGBRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=3,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "    def fit(self, X, time, event):\n",
    "        self.model.fit(X, -time)\n",
    "        return self\n",
    "        \n",
    "    def predict_risk(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "def evaluate_model(risk_scores, time, event):\n",
    "    return concordance_index(time, -risk_scores, event)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T06:43:21.353012Z",
     "start_time": "2024-11-08T06:43:20.905712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define feature types for clinical data only\n",
    "categorical_features = ['PATH_T_STAGE', 'CLIN_T_STAGE']\n",
    "numeric_clinical_features = ['GLEASON_SCORE', 'PRE_OPERATIVE_PSA', 'AGE']\n",
    "\n",
    "# First prepare clinical data\n",
    "clinical_data = pdata[categorical_features + numeric_clinical_features]\n",
    "clinical_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_clinical_features),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Transform clinical data\n",
    "X_clinical = clinical_preprocessor.fit_transform(clinical_data)\n",
    "\n",
    "# Scale expression data separately\n",
    "expression_scaler = StandardScaler()\n",
    "X_expression = expression_scaler.fit_transform(exprs)\n",
    "\n",
    "# Combine clinical and expression data\n",
    "X_combined = np.hstack([X_clinical, X_expression])\n",
    "\n",
    "time = pdata['MONTH_TO_BCR'].values\n",
    "event = pdata['BCR_STATUS'].values\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T06:44:14.470700Z",
     "start_time": "2024-11-08T06:44:00.648668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train and evaluate RSF\n",
    "rsf_model = SimpleRSF(n_estimators=100)\n",
    "rsf_model.fit(X_combined, time, event)\n",
    "rsf_risks = rsf_model.predict_risk(X_combined)\n",
    "rsf_cindex = evaluate_model(rsf_risks, time, event)\n",
    "print(f\"RSF C-index: {rsf_cindex:.3f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSF C-index: 0.940\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T07:01:31.939013Z",
     "start_time": "2024-11-08T07:01:05.389634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train and evaluate Gradient Boosting\n",
    "gb_model = SimpleGBSurvival(n_estimators=100)\n",
    "gb_model.fit(X_combined, time, event)\n",
    "gb_risks = gb_model.predict_risk(X_combined)\n",
    "gb_cindex = evaluate_model(gb_risks, time, event)\n",
    "print(f\"Gradient Boosting C-index: {gb_cindex:.3f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting C-index: 0.933\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T07:00:06.689423Z",
     "start_time": "2024-11-08T07:00:04.380021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, time_train, time_test, event_train, event_test = train_test_split(\n",
    "    X_combined, time, event, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=event  # Stratifizierung nach Events\n",
    ")\n",
    "\n",
    "class SimpleDeepSurv(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def train_deepsurv(X_train, X_val, time_train, time_val, event_train, event_val, \n",
    "                   epochs=50, lr=0.001, batch_size=32):\n",
    "    # Convert to tensors\n",
    "    X_train = torch.FloatTensor(X_train)\n",
    "    time_train = torch.FloatTensor(time_train)\n",
    "    event_train = torch.FloatTensor(event_train)\n",
    "    X_val = torch.FloatTensor(X_val)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = SimpleDeepSurv(X_train.shape[1])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_cindex = 0\n",
    "    best_model = None\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        risk_scores = model(X_train).squeeze()\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = negative_log_likelihood(risk_scores, time_train, event_train)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_risks = model(X_val).squeeze().numpy()\n",
    "            val_cindex = concordance_index(time_val, -val_risks, event_val)\n",
    "        model.train()\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(loss)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_cindex > best_val_cindex:\n",
    "            best_val_cindex = val_cindex\n",
    "            best_model = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "            \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, Val C-index: {val_cindex:.3f}')\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model is not None:\n",
    "        model.load_state_dict(best_model)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def predict_deepsurv(model, X):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        return model(torch.FloatTensor(X)).squeeze().numpy()\n",
    "\n",
    "# Print dataset sizes\n",
    "print(\"Dataset sizes:\")\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n",
    "print(f\"Training events: {event_train.sum()}\")\n",
    "print(f\"Test events: {event_test.sum()}\")\n",
    "\n",
    "# Train model\n",
    "deepsurv_model = train_deepsurv(X_train, X_test, time_train, time_test, event_train, event_test)\n",
    "\n",
    "# Evaluate on both train and test\n",
    "train_risks = predict_deepsurv(deepsurv_model, X_train)\n",
    "test_risks = predict_deepsurv(deepsurv_model, X_test)\n",
    "\n",
    "train_cindex = concordance_index(time_train, -train_risks, event_train)\n",
    "test_cindex = concordance_index(time_test, -test_risks, event_test)\n",
    "\n",
    "print(f\"\\nTraining C-index: {train_cindex:.3f}\")\n",
    "print(f\"Test C-index: {test_cindex:.3f}\")\n",
    "\n",
    "# Additional diagnostics on test set\n",
    "print(\"\\nTest Set Risk Score Distribution:\")\n",
    "print(f\"Min: {test_risks.min():.3f}\")\n",
    "print(f\"Max: {test_risks.max():.3f}\")\n",
    "print(f\"Mean: {test_risks.mean():.3f}\")\n",
    "print(f\"Std: {test_risks.std():.3f}\")\n",
    "\n",
    "# Compare risk scores for events vs non-events in test set\n",
    "test_event_risks = test_risks[event_test == 1]\n",
    "test_non_event_risks = test_risks[event_test == 0]\n",
    "print(\"\\nTest Set Mean Risk Scores:\")\n",
    "print(f\"Events: {test_event_risks.mean():.3f} ± {test_event_risks.std():.3f}\")\n",
    "print(f\"Non-Events: {test_non_event_risks.mean():.3f} ± {test_non_event_risks.std():.3f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "Training samples: 872\n",
      "Test samples: 219\n",
      "Training events: 241\n",
      "Test events: 60\n",
      "Epoch [5/50], Loss: 5.8293, Val C-index: 0.732\n",
      "Epoch [10/50], Loss: 5.4569, Val C-index: 0.757\n",
      "Epoch [15/50], Loss: 5.1732, Val C-index: 0.762\n",
      "Epoch [20/50], Loss: 4.9876, Val C-index: 0.763\n",
      "Epoch [25/50], Loss: 4.7655, Val C-index: 0.760\n",
      "Epoch [30/50], Loss: 4.5655, Val C-index: 0.764\n",
      "Epoch [35/50], Loss: 4.4006, Val C-index: 0.770\n",
      "Epoch [40/50], Loss: 4.2487, Val C-index: 0.776\n",
      "Epoch [45/50], Loss: 4.2363, Val C-index: 0.769\n",
      "Early stopping at epoch 49\n",
      "\n",
      "Training C-index: 0.982\n",
      "Test C-index: 0.765\n",
      "\n",
      "Test Set Risk Score Distribution:\n",
      "Min: -5.259\n",
      "Max: 4.633\n",
      "Mean: 0.109\n",
      "Std: 1.853\n",
      "\n",
      "Test Set Mean Risk Scores:\n",
      "Events: 1.395 ± 1.657\n",
      "Non-Events: -0.376 ± 1.683\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting\n",
    "performed better with merged data than with intersection\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "def prepare_clinical_features(pdata):\n",
    "    \"\"\"\n",
    "    Enhanced clinical feature preparation\n",
    "    \"\"\"\n",
    "    # Add more potentially relevant clinical features\n",
    "    clinical_features = [\n",
    "        'GLEASON_SCORE',\n",
    "        'PATH_T_STAGE', 'CLIN_T_STAGE', \n",
    "        'PRE_OPERATIVE_PSA', 'AGE'\n",
    "    ]\n",
    "    \n",
    "    clinical_data = pdata[clinical_features].copy()\n",
    "    \n",
    "    # Enhanced categorical handling\n",
    "    categorical_features = ['PATH_T_STAGE', 'CLIN_T_STAGE']\n",
    "    clinical_data = pd.get_dummies(\n",
    "        clinical_data, \n",
    "        columns=categorical_features, \n",
    "        drop_first=True,\n",
    "        prefix=categorical_features\n",
    "    )\n",
    "    \n",
    "    # More sophisticated missing value imputation\n",
    "    numerical_columns = ['GLEASON_SCORE', \n",
    "                        'PRE_OPERATIVE_PSA', 'AGE']\n",
    "    \n",
    "    # Use different strategies for different types of missing values\n",
    "    for col in numerical_columns:\n",
    "        if col in ['GLEASON_SCORE']:\n",
    "            clinical_data[col] = clinical_data[col].fillna(clinical_data[col].mode()[0])\n",
    "        else:\n",
    "            clinical_data[col] = clinical_data[col].fillna(clinical_data[col].median())\n",
    "    \n",
    "    # Add interaction terms for important clinical features\n",
    "    clinical_data['GLEASON_PSA_INTERACTION'] = clinical_data['GLEASON_SCORE'] * clinical_data['PRE_OPERATIVE_PSA']\n",
    "    \n",
    "    return clinical_data\n",
    "\n",
    "def select_important_genes(exprs_data, y, n_features=1000):\n",
    "    \"\"\"\n",
    "    Pre-select important genetic features\n",
    "    \"\"\"\n",
    "    selector = SelectFromModel(\n",
    "        GradientBoostingSurvivalAnalysis(\n",
    "            n_estimators=500,\n",
    "            learning_rate=0.01,\n",
    "            max_depth=3\n",
    "        ),\n",
    "        max_features=n_features\n",
    "    )\n",
    "    selector.fit(exprs_data, y)\n",
    "    return selector.transform(exprs_data), selector.get_feature_names_out()\n",
    "\n",
    "def run_boosting_comparison_with_clinical(exprs_data, pdata, survival_data):\n",
    "    \"\"\"\n",
    "    Enhanced gradient boosting with extensive tuning\n",
    "    \"\"\"\n",
    "    # Prepare clinical features\n",
    "    clinical_data = prepare_clinical_features(pdata)\n",
    "    \n",
    "    # Create survival array\n",
    "    y = np.array([(status, time) for status, time in \n",
    "                  zip(survival_data['BCR_STATUS'], survival_data['MONTH_TO_BCR'])],\n",
    "                 dtype=[('status', bool), ('time', float)])\n",
    "    \n",
    "    # Pre-select important genes\n",
    "    selected_exprs, selected_genes = select_important_genes(exprs_data, y)\n",
    "    \n",
    "    # Combine features\n",
    "    combined_features = pd.concat([\n",
    "        pd.DataFrame(selected_exprs, index=exprs_data.index, columns=selected_genes),\n",
    "        clinical_data\n",
    "    ], axis=1)\n",
    "    \n",
    "    # Create cross-validation strategy\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', RobustScaler()),\n",
    "        ('gb', GradientBoostingSurvivalAnalysis())\n",
    "    ])\n",
    "    \n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'gb__n_estimators': [500, 1000],\n",
    "        'gb__learning_rate': [0.01, 0.005],\n",
    "        'gb__max_depth': [3, 5],\n",
    "        'gb__min_samples_split': [5, 10],\n",
    "        'gb__min_samples_leaf': [3, 5],\n",
    "        'gb__subsample': [0.8, 0.8],\n",
    "        'gb__max_features': ['sqrt', None]\n",
    "    }\n",
    "    \n",
    "    # Perform grid search\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        scoring=lambda estimator, X, y: concordance_index_censored(\n",
    "            y['status'],\n",
    "            y['time'],\n",
    "            estimator.predict(X)\n",
    "        )[0],\n",
    "        n_jobs=-1,  # Use all available cores\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        combined_features, y, test_size=0.2, random_state=42, stratify=y['status']\n",
    "    )\n",
    "    \n",
    "    # Fit grid search\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Get predictions\n",
    "    pred_risk = best_model.predict(X_test)\n",
    "    c_index = concordance_index_censored(y_test['status'], y_test['time'], pred_risk)[0]\n",
    "    \n",
    "    # Get feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': combined_features.columns,\n",
    "        'importance': best_model.named_steps['gb'].feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Print detailed results\n",
    "    print(\"\\nBest parameters:\", grid_search.best_params_)\n",
    "    print(\"\\nCross-validation results:\")\n",
    "    cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "    print(cv_results[['params', 'mean_test_score', 'std_test_score']])\n",
    "    \n",
    "    return best_model, c_index, feature_importance, grid_search\n",
    "\n",
    "# Run with enhanced monitoring\n",
    "model, c_index, feature_importance, grid_search = run_boosting_comparison_with_clinical(\n",
    "    exprs, pdata, survival_data\n",
    ")\n",
    "\n",
    "# Enhanced visualization\n",
    "def plot_detailed_results(feature_importance, grid_search, c_index):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Feature importance plot\n",
    "    plt.subplot(2, 1, 1)\n",
    "    top_features = feature_importance.head(20)\n",
    "    plt.barh(top_features['feature'], top_features['importance'])\n",
    "    plt.title(f'Top 20 Most Important Features (C-index: {c_index:.4f})')\n",
    "    plt.xlabel('Importance')\n",
    "    \n",
    "    # Learning curves\n",
    "    plt.subplot(2, 1, 2)\n",
    "    cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "    plt.plot(cv_results['mean_test_score'], label='Mean CV Score')\n",
    "    plt.fill_between(\n",
    "        range(len(cv_results)),\n",
    "        cv_results['mean_test_score'] - cv_results['std_test_score'],\n",
    "        cv_results['mean_test_score'] + cv_results['std_test_score'],\n",
    "        alpha=0.3\n",
    "    )\n",
    "    plt.title('Cross-validation Learning Curves')\n",
    "    plt.xlabel('Parameter Combination')\n",
    "    plt.ylabel('C-index')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_detailed_results(feature_importance, grid_search, c_index)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Including Batch effects. Curretnly not trained with sufficent params but interestingly Gleason Score is now relevant\n",
    "C-Index:0,739, dauert 750min"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class BatchAwareSurvivalModel:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.base_model = GradientBoostingSurvivalAnalysis(\n",
    "            n_estimators=1000,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=3\n",
    "        )\n",
    "        \n",
    "    def _extract_batch_info(self, data):\n",
    "        \"\"\"Extrahiert Batch-Information aus den Index-Namen\"\"\"\n",
    "        return data.index.map(lambda x: x.split('.')[0])\n",
    "    \n",
    "    def _center_within_batch(self, X, batch_ids):\n",
    "        \"\"\"Zentriert Features innerhalb jedes Batches\"\"\"\n",
    "        X_centered = X.copy()\n",
    "        for batch in np.unique(batch_ids):\n",
    "            mask = batch_ids == batch\n",
    "            X_centered.loc[mask] = self.scaler.fit_transform(X.loc[mask])\n",
    "        return X_centered\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: Feature-Matrix (gene expression + clinical)\n",
    "        y: Survival data (status, time)\n",
    "        \"\"\"\n",
    "        # Batch-Information extrahieren\n",
    "        batch_ids = self._extract_batch_info(X)\n",
    "        \n",
    "        # Batch-Korrektur durchführen\n",
    "        X_corrected = self._center_within_batch(X, batch_ids)\n",
    "        \n",
    "        # Basis-Modell fitten\n",
    "        self.base_model.fit(X_corrected, y)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # Batch-Korrektur für Vorhersage\n",
    "        batch_ids = self._extract_batch_info(X)\n",
    "        X_corrected = self._center_within_batch(X, batch_ids)\n",
    "        return self.base_model.predict(X_corrected)\n",
    "    \n",
    "    def get_feature_importance(self):\n",
    "        return pd.Series(\n",
    "            self.base_model.feature_importances_,\n",
    "            index=self.base_model.feature_names_in_\n",
    "        )\n",
    "\n",
    "def evaluate_model_with_cv(X, y, n_splits=5):\n",
    "    \"\"\"Evaluiert das Modell mit Cross-Validation\"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    c_indices = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "        # Daten für diesen Fold aufteilen\n",
    "        X_train = X.iloc[train_idx]\n",
    "        X_test = X.iloc[test_idx]\n",
    "        y_train = y[train_idx]\n",
    "        y_test = y[test_idx]\n",
    "        \n",
    "        # Modell trainieren und evaluieren\n",
    "        model = BatchAwareSurvivalModel()\n",
    "        model.fit(X_train, y_train)\n",
    "        pred = model.predict(X_test)\n",
    "        \n",
    "        # C-Index berechnen\n",
    "        c_index = concordance_index_censored(\n",
    "            y_test['status'],\n",
    "            y_test['time'],\n",
    "            pred\n",
    "        )[0]\n",
    "        \n",
    "        c_indices.append(c_index)\n",
    "        print(f\"Fold {fold+1} C-index: {c_index:.3f}\")\n",
    "    \n",
    "    return np.mean(c_indices), np.std(c_indices)\n",
    "\n",
    "def plot_results(feature_importance, c_index_mean, c_index_std):\n",
    "    \"\"\"Visualisiert die Ergebnisse\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Top 20 wichtigste Features\n",
    "    top_features = feature_importance.sort_values(ascending=True)[-20:]\n",
    "    \n",
    "    plt.barh(range(len(top_features)), top_features)\n",
    "    plt.yticks(range(len(top_features)), top_features.index)\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(f'Top 20 Features (C-index: {c_index_mean:.3f} ± {c_index_std:.3f})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Hauptausführung\n",
    "def main():\n",
    "    # Daten laden\n",
    "    exprs = loader.common_genes_data['common_genes_knn_imputed.csv']\n",
    "    pdata = loader.merged_pdata_imputed['merged_imputed_pData.csv']\n",
    "    \n",
    "    # Survival Array erstellen\n",
    "    y = np.array([(status, time) for status, time in \n",
    "                  zip(pdata['BCR_STATUS'], pdata['MONTH_TO_BCR'])],\n",
    "                 dtype=[('status', bool), ('time', float)])\n",
    "    \n",
    "    # Klinische Features vorbereiten\n",
    "    clinical_features = ['GLEASON_SCORE', 'PATH_T_STAGE', 'PRE_OPERATIVE_PSA']\n",
    "    clinical_data = pd.get_dummies(\n",
    "        pdata[clinical_features], \n",
    "        columns=['PATH_T_STAGE']\n",
    "    )\n",
    "    \n",
    "    # Genetische und klinische Daten kombinieren\n",
    "    combined_features = pd.concat([exprs, clinical_data], axis=1)\n",
    "    \n",
    "    # Modell evaluieren\n",
    "    c_index_mean, c_index_std = evaluate_model_with_cv(combined_features, y)\n",
    "    \n",
    "    # Finales Modell für Feature Importance\n",
    "    final_model = BatchAwareSurvivalModel()\n",
    "    final_model.fit(combined_features, y)\n",
    "    feature_importance = final_model.get_feature_importance()\n",
    "    \n",
    "    # Ergebnisse plotten\n",
    "    plot_results(feature_importance, c_index_mean, c_index_std)\n",
    "    \n",
    "    print(f\"\\nOverall C-index: {c_index_mean:.3f} (±{c_index_std:.3f})\")\n",
    "    print(\"\\nTop 10 most important features:\")\n",
    "    print(feature_importance.sort_values(ascending=False).head(10))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from preprocessing import DataLoader\n",
    "    import os\n",
    "    \n",
    "    # DataLoader initialisieren\n",
    "    base_path = os.path.dirname(os.getcwd())\n",
    "    loader = DataLoader(base_path)\n",
    "    loader.load_all_data()\n",
    "    \n",
    "    main()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemle mit RSF und Compontent wise GB 0,67\n",
    "(Params noch nicht gefixed)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "class SurvivalEnsemble:\n",
    "    def __init__(self):\n",
    "        # Initialisiere beide Modelle mit einfachen Parametern\n",
    "        self.rsf = RandomSurvivalForest(\n",
    "            n_estimators=1000,\n",
    "            max_depth=4,\n",
    "            n_jobs=-1,\n",
    "            random_state=42,\n",
    "            min_samples_split=5\n",
    "        )\n",
    "        \n",
    "        self.cgb = ComponentwiseGradientBoostingSurvivalAnalysis(\n",
    "            n_estimators=1000,\n",
    "            learning_rate=0.01,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Standardisiere Features\n",
    "        X_scaled = pd.DataFrame(\n",
    "            self.scaler.fit_transform(X),\n",
    "            index=X.index,\n",
    "            columns=X.columns\n",
    "        )\n",
    "        \n",
    "        # Fitte beide Modelle\n",
    "        self.rsf.fit(X_scaled, y)\n",
    "        self.cgb.fit(X_scaled, y)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # Standardisiere Features\n",
    "        X_scaled = pd.DataFrame(\n",
    "            self.scaler.transform(X),\n",
    "            index=X.index,\n",
    "            columns=X.columns\n",
    "        )\n",
    "        \n",
    "        # Hole Vorhersagen\n",
    "        pred_rsf = self.rsf.predict(X_scaled)\n",
    "        pred_cgb = self.cgb.predict(X_scaled)\n",
    "        \n",
    "        # Kombiniere Vorhersagen (einfacher Durchschnitt)\n",
    "        return (pred_rsf + pred_cgb) / 2\n",
    "\n",
    "def evaluate_ensemble():\n",
    "    # Daten laden\n",
    "    exprs = loader.common_genes_data['common_genes_knn_imputed.csv']\n",
    "    pdata = loader.merged_pdata_imputed['merged_imputed_pData.csv']\n",
    "    \n",
    "    # Survival Array erstellen\n",
    "    y = np.array([(status, time) for status, time in \n",
    "                  zip(pdata['BCR_STATUS'], pdata['MONTH_TO_BCR'])],\n",
    "                 dtype=[('status', bool), ('time', float)])\n",
    "    \n",
    "    # Klinische Features vorbereiten\n",
    "    clinical_features = ['GLEASON_SCORE', 'PATH_T_STAGE', 'PRE_OPERATIVE_PSA']\n",
    "    clinical_data = pd.get_dummies(\n",
    "        pdata[clinical_features], \n",
    "        columns=['PATH_T_STAGE']\n",
    "    )\n",
    "    \n",
    "    # Features kombinieren\n",
    "    X = pd.concat([exprs, clinical_data], axis=1)\n",
    "    \n",
    "    # Cross-Validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    c_indices = []\n",
    "    \n",
    "    # Progress Tracking\n",
    "    total_folds = 5\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X), 1):\n",
    "        print(f\"\\nStarting Fold {fold}/{total_folds} ({fold/total_folds*100:.1f}% complete)\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train = X.iloc[train_idx]\n",
    "        X_test = X.iloc[test_idx]\n",
    "        y_train = y[train_idx]\n",
    "        y_test = y[test_idx]\n",
    "        \n",
    "        # Train ensemble\n",
    "        print(\"Training Random Survival Forest...\")\n",
    "        ensemble = SurvivalEnsemble()\n",
    "        ensemble.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        print(\"Making predictions...\")\n",
    "        pred = ensemble.predict(X_test)\n",
    "        \n",
    "        # Calculate c-index\n",
    "        c_index = concordance_index_censored(\n",
    "            y_test['status'],\n",
    "            y_test['time'],\n",
    "            pred\n",
    "        )[0]\n",
    "        \n",
    "        c_indices.append(c_index)\n",
    "        print(f\"Fold {fold} C-index: {c_index:.3f}\")\n",
    "    \n",
    "    # Final results\n",
    "    mean_c_index = np.mean(c_indices)\n",
    "    std_c_index = np.std(c_indices)\n",
    "    \n",
    "    print(\"\\nFinal Results:\")\n",
    "    print(f\"Mean C-index: {mean_c_index:.3f} (±{std_c_index:.3f})\")\n",
    "    print(\"Individual fold C-indices:\", c_indices)\n",
    "    \n",
    "    return mean_c_index, std_c_index\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from preprocessing import DataLoader\n",
    "    import os\n",
    "    \n",
    "    # DataLoader initialisieren\n",
    "    base_path = os.path.dirname(os.getcwd())\n",
    "    loader = DataLoader(base_path)\n",
    "    loader.load_all_data()\n",
    "    \n",
    "    # Daten laden\n",
    "    common_genes_data = loader.common_genes_data\n",
    "    \n",
    "    # Evaluation durchführen\n",
    "    mean_c_index, std_c_index = evaluate_ensemble()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi Task Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "\n",
    "class DeepMTLSurvival(nn.Module):\n",
    "    def __init__(self, clinical_dim, genomic_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Clinical pathway\n",
    "        self.clinical_net = nn.Sequential(\n",
    "            nn.Linear(clinical_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # Genomic pathway\n",
    "        self.genomic_net = nn.Sequential(\n",
    "            nn.Linear(genomic_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # Combined pathway\n",
    "        self.combined_net = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Output heads\n",
    "        self.time_out = nn.Linear(16, 1)\n",
    "        self.event_out = nn.Linear(16, 1)\n",
    "        \n",
    "    def forward(self, clinical, genomic):\n",
    "        # Process clinical data\n",
    "        clinical_features = self.clinical_net(clinical)\n",
    "        \n",
    "        # Process genomic data\n",
    "        genomic_features = self.genomic_net(genomic)\n",
    "        \n",
    "        # Combine features\n",
    "        combined = torch.cat([clinical_features, genomic_features], dim=1)\n",
    "        shared_features = self.combined_net(combined)\n",
    "        \n",
    "        # Generate predictions\n",
    "        time_pred = self.time_out(shared_features)\n",
    "        event_pred = torch.sigmoid(self.event_out(shared_features))\n",
    "        \n",
    "        return time_pred, event_pred, clinical_features, genomic_features\n",
    "\n",
    "class MTLSurvivalLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.time_criterion = nn.MSELoss()\n",
    "        self.event_criterion = nn.BCELoss()\n",
    "        \n",
    "    def forward(self, time_pred, event_pred, time_true, event_true, \n",
    "                time_pred_b=None, event_pred_b=None, \n",
    "                time_true_b=None, event_true_b=None, lam=1.0):\n",
    "        if time_pred_b is None:  # No mixup\n",
    "            time_loss = self.time_criterion(time_pred, time_true.view(-1, 1))\n",
    "            event_loss = self.event_criterion(event_pred, event_true.view(-1, 1).float())\n",
    "            return time_loss + event_loss\n",
    "        else:  # Mixup\n",
    "            time_loss = lam * self.time_criterion(time_pred, time_true.view(-1, 1)) + \\\n",
    "                       (1 - lam) * self.time_criterion(time_pred_b, time_true_b.view(-1, 1))\n",
    "            event_loss = lam * self.event_criterion(event_pred, event_true.view(-1, 1).float()) + \\\n",
    "                        (1 - lam) * self.event_criterion(event_pred_b, event_true_b.view(-1, 1).float())\n",
    "            return time_loss + event_loss\n",
    "\n",
    "class SurvivalDataset(Dataset):\n",
    "    def __init__(self, clinical_data, genomic_data, time, event):\n",
    "        self.clinical_data = torch.FloatTensor(clinical_data)\n",
    "        self.genomic_data = torch.FloatTensor(genomic_data)\n",
    "        self.time = torch.FloatTensor(time)\n",
    "        self.event = torch.FloatTensor(event)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.time)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'clinical': self.clinical_data[idx],\n",
    "            'genomic': self.genomic_data[idx],\n",
    "            'time': self.time[idx],\n",
    "            'event': self.event[idx]\n",
    "        }\n",
    "\n",
    "class DeepMTLSurvivalAnalysis:\n",
    "    def __init__(self, clinical_dim, genomic_dim, device='cuda', **kwargs):\n",
    "        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        self.model = DeepMTLSurvival(clinical_dim, genomic_dim).to(self.device)\n",
    "        self.criterion = MTLSurvivalLoss()\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(),\n",
    "            lr=kwargs.get('lr', 0.001),\n",
    "            weight_decay=kwargs.get('weight_decay', 1e-5)\n",
    "        )\n",
    "    \n",
    "    def mixup_data(self, clinical, genomic, time, event, alpha=0.2):\n",
    "        if alpha > 0:\n",
    "            lam = np.random.beta(alpha, alpha)\n",
    "        else:\n",
    "            lam = 1\n",
    "\n",
    "        batch_size = clinical.size()[0]\n",
    "        index = torch.randperm(batch_size).to(self.device)\n",
    "\n",
    "        mixed_clinical = lam * clinical + (1 - lam) * clinical[index]\n",
    "        mixed_genomic = lam * genomic + (1 - lam) * genomic[index]\n",
    "        \n",
    "        return mixed_clinical, mixed_genomic, time, time[index], event, event[index], lam\n",
    "        \n",
    "    def fit(self, train_loader, val_loader=None, epochs=100):\n",
    "        print(\"Starting training...\")\n",
    "        self.scheduler = OneCycleLR(\n",
    "            self.optimizer,\n",
    "            max_lr=0.001,\n",
    "            epochs=epochs,\n",
    "            steps_per_epoch=len(train_loader),\n",
    "            pct_start=0.3\n",
    "        )\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            \n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                clinical = batch['clinical'].to(self.device)\n",
    "                genomic = batch['genomic'].to(self.device)\n",
    "                time = batch['time'].to(self.device)\n",
    "                event = batch['event'].to(self.device)\n",
    "                \n",
    "                if np.random.random() > 0.5:\n",
    "                    (clinical_mixed, genomic_mixed, \n",
    "                     time_a, time_b, \n",
    "                     event_a, event_b, \n",
    "                     lam) = self.mixup_data(clinical, genomic, \n",
    "                                          time, event)\n",
    "                    \n",
    "                    time_pred_mixed, event_pred_mixed, _, _ = self.model(\n",
    "                        clinical_mixed, genomic_mixed\n",
    "                    )\n",
    "                    \n",
    "                    loss = self.criterion(\n",
    "                        time_pred_mixed, event_pred_mixed,\n",
    "                        time_a, event_a,\n",
    "                        time_pred_mixed, event_pred_mixed,\n",
    "                        time_b, event_b,\n",
    "                        lam\n",
    "                    )\n",
    "                else:\n",
    "                    time_pred, event_pred, _, _ = self.model(\n",
    "                        clinical, genomic\n",
    "                    )\n",
    "                    \n",
    "                    loss = self.criterion(time_pred, event_pred, time, event)\n",
    "                \n",
    "                loss.backward()\n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.model.parameters(), \n",
    "                    max_norm=1.0\n",
    "                )\n",
    "                \n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                \n",
    "                if batch_idx % 10 == 0:\n",
    "                    print(f'Epoch: {epoch+1}/{epochs}, Batch: {batch_idx}/{len(train_loader)}, '\n",
    "                          f'Loss: {loss.item():.4f}')\n",
    "            \n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            print(f'Epoch {epoch+1}/{epochs} - Average Training Loss: {avg_train_loss:.4f}')\n",
    "            \n",
    "            if val_loader is not None:\n",
    "                val_loss = self.evaluate(val_loader)\n",
    "                print(f'Validation Loss: {val_loss:.4f}')\n",
    "                \n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                \n",
    "                if patience_counter >= 15:\n",
    "                    print(\"Early stopping triggered\")\n",
    "                    break\n",
    "                \n",
    "    def predict(self, loader):\n",
    "        print(\"Making predictions...\")\n",
    "        self.model.eval()\n",
    "        time_preds = []\n",
    "        event_preds = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                clinical = batch['clinical'].to(self.device)\n",
    "                genomic = batch['genomic'].to(self.device)\n",
    "                \n",
    "                time_pred, event_pred, _, _ = self.model(clinical, genomic)\n",
    "                \n",
    "                time_preds.append(time_pred.cpu())\n",
    "                event_preds.append(event_pred.cpu())\n",
    "        \n",
    "        return (\n",
    "            torch.cat(time_preds).numpy(),\n",
    "            torch.cat(event_preds).numpy()\n",
    "        )\n",
    "    \n",
    "    def evaluate(self, loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                clinical = batch['clinical'].to(self.device)\n",
    "                genomic = batch['genomic'].to(self.device)\n",
    "                time = batch['time'].to(self.device)\n",
    "                event = batch['event'].to(self.device)\n",
    "                \n",
    "                time_pred, event_pred, _, _ = self.model(clinical, genomic)\n",
    "                \n",
    "                loss = self.criterion(time_pred, event_pred, time, event)\n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        return total_loss / len(loader)\n",
    "\n",
    "def evaluate_mtl_survival(data_loader):\n",
    "    print(\"Starting evaluation...\")\n",
    "    \n",
    "    # Load data\n",
    "    exprs = data_loader.common_genes_data['common_genes_knn_imputed.csv']\n",
    "    pdata = data_loader.merged_pdata_imputed['merged_imputed_pData.csv']\n",
    "    \n",
    "    print(f\"Data loaded - Expression shape: {exprs.shape}, Clinical shape: {pdata.shape}\")\n",
    "    \n",
    "    clinical_features = ['GLEASON_SCORE', 'PATH_T_STAGE', 'PRE_OPERATIVE_PSA']\n",
    "    clinical_data = pd.get_dummies(\n",
    "        pdata[clinical_features], \n",
    "        columns=['PATH_T_STAGE']\n",
    "    )\n",
    "    \n",
    "    print(f\"Clinical features after one-hot encoding: {clinical_data.columns.tolist()}\")\n",
    "    \n",
    "    clinical_scaler = StandardScaler()\n",
    "    genomic_scaler = StandardScaler()\n",
    "    \n",
    "    X_clinical = clinical_scaler.fit_transform(clinical_data)\n",
    "    X_genomic = genomic_scaler.fit_transform(exprs)\n",
    "    \n",
    "    time = pdata['MONTH_TO_BCR'].values\n",
    "    event = pdata['BCR_STATUS'].values\n",
    "    \n",
    "    print(f\"Data prepared - Clinical shape: {X_clinical.shape}, Genomic shape: {X_genomic.shape}\")\n",
    "    print(f\"Time shape: {time.shape}, Event shape: {event.shape}\")\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    c_indices = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X_clinical), 1):\n",
    "        print(f\"\\nProcessing Fold {fold}/5\")\n",
    "        \n",
    "        X_clinical_train = X_clinical[train_idx]\n",
    "        X_clinical_test = X_clinical[test_idx]\n",
    "        X_genomic_train = X_genomic[train_idx]\n",
    "        X_genomic_test = X_genomic[test_idx]\n",
    "        time_train = time[train_idx]\n",
    "        time_test = time[test_idx]\n",
    "        event_train = event[train_idx]\n",
    "        event_test = event[test_idx]\n",
    "        \n",
    "        print(f\"Training set sizes - Clinical: {X_clinical_train.shape}, Genomic: {X_genomic_train.shape}\")\n",
    "        print(f\"Test set sizes - Clinical: {X_clinical_test.shape}, Genomic: {X_genomic_test.shape}\")\n",
    "        \n",
    "        train_dataset = SurvivalDataset(\n",
    "            X_clinical_train, X_genomic_train, \n",
    "            time_train, event_train\n",
    "        )\n",
    "        test_dataset = SurvivalDataset(\n",
    "            X_clinical_test, X_genomic_test,\n",
    "            time_test, event_test\n",
    "        )\n",
    "        \n",
    "        train_loader = TorchDataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=64,\n",
    "            shuffle=True,\n",
    "            drop_last=True\n",
    "        )\n",
    "        test_loader = TorchDataLoader(\n",
    "            test_dataset, \n",
    "            batch_size=64,\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        model = DeepMTLSurvivalAnalysis(\n",
    "            clinical_dim=X_clinical.shape[1],\n",
    "            genomic_dim=X_genomic.shape[1],\n",
    "            lr=0.001,\n",
    "            weight_decay=1e-5\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nTraining model for fold {fold}\")\n",
    "        model.fit(train_loader, epochs=100)\n",
    "        \n",
    "        print(f\"\\nMaking predictions for fold {fold}\")\n",
    "        time_pred, _ = model.predict(test_loader)\n",
    "        \n",
    "        c_index = concordance_index_censored(\n",
    "            event_test.astype(bool),\n",
    "            time_test,\n",
    "            -time_pred.squeeze()\n",
    "        )[0]\n",
    "        \n",
    "        c_indices.append(c_index)\n",
    "        print(f\"Fold {fold} C-index: {c_index:.3f}\")\n",
    "    \n",
    "    print(\"\\nFinal Results:\")\n",
    "    print(f\"Mean C-index: {np.mean(c_indices):.3f} (±{np.std(c_indices):.3f})\")\n",
    "    print(\"Individual fold C-indices:\", c_indices)\n",
    "    \n",
    "    return np.mean(c_indices), np.std(c_indices)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from preprocessing import DataLoader\n",
    "    import os\n",
    "    \n",
    "    print(\"Starting program...\")\n",
    "    base_path = os.path.dirname(os.getcwd())\n",
    "    print(f\"Base path: {base_path}\")\n",
    "    \n",
    "    loader = DataLoader(base_path)\n",
    "    print(\"DataLoader initialized\")\n",
    "    \n",
    "    loader.load_all_data()\n",
    "    print(\"All data loaded\")\n",
    "    \n",
    "    mean_c_index, std_c_index = evaluate_mtl_survival(loader)\n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"Mean C-index: {mean_c_index:.3f} (±{std_c_index:.3f})\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (StatisticalConsulting)",
   "language": "python",
   "name": "statisticalconsulting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
